{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: autoencoder是一种无监督的学习算法，主要用于数据的降维或者特征的抽取，在深度学习中，autoencoder可用于在训练阶段开始前，确定权重矩阵的初始值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A：greedy search 即decoder的每一步都选择最可能的单词，最后得到句子的每一个单词都是每一步认为最合适的单词。\n",
    "   beam search 每步选取最可能的kk个结果，再从最后的kk个结果中选取最合适的句子。kk称为beam size。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A： attention机制的直观认识是：人在进行翻译的时候，不会在翻译某个词或者词组的时候把整个句子都看一遍，一般来说，都是通过关注这个词本身以及离这个词最近的一些词，然后再进行翻译。也就是说，我们会把焦点放在当前词所在的局部范围。而attention机制就是通过给予对当前输出位置比较重要(比较近)的输入位置较大的权重，较远的位置给予较小的权重来做到局部聚焦的作用。另外，由于attention是直接和input接触，所以也能做到长程联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:之前介绍的word embeding其中一个缺陷就是无法处理同义词。因为训练的时候是输入全部的语料库，输出的是每一个词的词向量，如果一个词有同义词，那么这个词的词向量其实是所有同义词的词向量混合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 简单的说，ELMo模型先对语料库进行双层双向LSTM预训练，然后对训练完的ELMo网络输入新句子，句子中每个词都获得了对应的3层word embedding(底层word embedding，第一层LSTM embedding和第二层LSTM embedding)，通过学习到的权重加权累加得到最后的word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 语义特征提取能力强于RNN；从综合特征抽取能力角度衡量，Transformer 显著强于RNN；并行计算能力及运算效率强于RNN  \n",
    "+   1.RNN每个输入的计算都依赖于前一个输入的输出，无法进行并行计算，而Transformer不存在这种序列依赖，所有输入能同步进行计算。  \n",
    "+   2.RNN在建立长程联系的时候会出现bias或者梯度消失的问题，而Transformer可以通过self-attention让每个词和所有词进行直接交互，建立直接依赖，无论距离多远。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；LN不依赖于batch的大小和输入sequence的深度\n",
    "  \n",
    "  batch normalization其实是对一个Batch中每一个句子的同一位置的词的词向量进行normalization，但是事实上每个句子同一位置的词一般都不同，意思也不一样，这样进行normalization完全没有意义；\n",
    "  而layer normalization是对每个词自己的词向量维度进行normalization，不存在这种问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: 因为Transformer里只有attention，而attention不像RNN会有一个时序的差异，attention对所有的(query, key)的计算是完全一样的，所以为了体现input的时序或位置的差异，需要额外输入一个position信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: self-attention计算每个单词与其他所有单词之间的关联。multi-head attention 由多个 scaled dot-product attention 这样的基础单元经过 stack 而成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Masked Multi-head self-attention + Layer Norm + Feed Forward + Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + 1.对于分类问题(classification)，可以直接套用GPT模型: Input + Transformer + Linear + 分类层\n",
    "+  2.对于文本蕴含(Entailment)，需要对输入进行调整，即用分隔符(Delim)拼接前提p和假设h作为Input，加上Transformer和Linear\n",
    "+ 3.文本相似度(Simlilarity)，把Input1和Input2用分隔符(Delim)按前后顺序拼接成Input1 Delim Input2和Input2 Delim Input1然后分别进入Transformer后将结果相加，再加上Linear层\n",
    "+ 4.多选题(Multiple Choice)，把Question分别和每一个Answer用分隔符拼接起来，输入到Transformer+Linear层，得到N个结果通过softmax层输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Masked language model是随机去掉部分(15%)token，然后模型来预测去掉到token是什么。\n",
    "\n",
    "具体的操作是：80%的时间中，选中的词被Mask；10%的时间中，选中的词用任意词替代；10%的时间中，选中的词不变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">BERT的输入为3个embedding的求和：\n",
    "+ Token embedding表示当前词的embedding\n",
    "+ Segment embedding表示所在句子的index embedding\n",
    "+ Position embedding表示当前词所在位置的index embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">+ 在分类问题中，只要在BERT输出层加个分类层就行\n",
    "+ 在QA问题中，需要接受文本序列并且标注Answer，然后用BERT学习两个标注Answer开始和结束的向量来训练QA模型\n",
    "+ 在命名实体识别(NER)中，需要对系统的文本中的各个实体进行标注(如人名，地点，时间)，然后将每个token的输出向量送到预测NER标签的分类层中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GPT2 与 GPT 的大致模型框架和预训练目标是一致的，而区别主要在于以下几个方面：\n",
    "\n",
    "其使用了更大的模型\n",
    "使用了数量更大、质量更高、涵盖范围更广的预训练数据\n",
    "采用了无监督多任务联合训练的方式，即对于输入样本，给予一个该样本所属的类别作为引导字符串，这使得该模型能够同时对多项任务进行联合训练，并增强模型的泛化能力\n",
    "\n",
    "+ GPT: 结构为单向+Transformer，输入为Text+Pos embedding\n",
    "+ BERT: 结构为双向+Transformer，输入为word，segment+position embedding\n",
    "+ GPT2: 沿用GPT模型，但是放弃Fine-Tuning过程，用更大的数据集，更大的网络容量，并且调整网络结构，做到无监督训练的通用语言模型去完成不同的任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
