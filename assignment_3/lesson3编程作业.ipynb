{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵 $$Entropy = -\\sum_i^n Pr(x_i) log(Pr(x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(elements):  #群体的混乱程度\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_min_spliter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_1)\n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            ic(sub_spliter_2)\n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_2)\n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1, 1, 1], [1, 0, 1], [1, 1, 2], [1, 1, 1], [0, 1, 1], [0, 1, 1], [0, 0, 2]]\n",
    "Y = [1, 1, 1, 0, 0, 0, 1]\n",
    "# X 和 Y 对应 dataset列表\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "clf.predict([[0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate(gender, income, family_number):\n",
    "    if gender == 'M':\n",
    "        a = 0\n",
    "    a=1\n",
    "    if income == +10:\n",
    "        b = 1\n",
    "    b=0  \n",
    "    return clf.predict([[a, b, family_number]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate('M', -10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "别人写的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure find_best_spliter function\n",
    "def best_spliter(training_data, features, target: str) -> str:\n",
    "    \"\"\"Sort the features by salience (defined by entropy) and return the best feature in string\n",
    "    Args:\n",
    "        training_data: the training data (xi, yi)\n",
    "        features: unused spliters\n",
    "        target: the target feature or element of data\n",
    "    Returns:\n",
    "        the best feature of training data\n",
    "    \"\"\"\n",
    "    spliters = list()\n",
    "    \n",
    "    for f in features:\n",
    "        values = set(training_data[f])\n",
    "        entropy_v = sum(entropy(training_data[training_data[f] == v][target].tolist()) for v in values)\n",
    "        spliters.append([f, values, entropy_v])\n",
    "    \n",
    "    spliters.sort(key=lambda x:x[2])\n",
    "    return spliters[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(training_data: pd.DataFrame, target:str, criterion=entropy):\n",
    "    \"\"\"Fit the data on decision model and return the predict function\n",
    "    Args: \n",
    "        training_data: the data in pd.DataFrame structure\n",
    "        target: the target column's title\n",
    "        criterion: the function to sort the feature. Default is entropy I defined before\n",
    "    Returns:\n",
    "        predict function\n",
    "    \"\"\"\n",
    "    features = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    def get_tree(data, features_unused):\n",
    "        spliter = best_spliter(data, features_unused, target) # get the best spliter\n",
    "        values = set(training_data[spliter])\n",
    "        features_unused -= {spliter}\n",
    "        return [spliter] + [get_details(spliter, value, data,features_unused) for value in values]\n",
    "    \n",
    "    def get_details(spliter, value, data, features_unused):\n",
    "        entropy = criterion(data[data[spliter] == value][target].tolist())\n",
    "        # entropy == 0, return the target value of this group\n",
    "        if entropy == 0:\n",
    "            return [value, entropy, data[data[spliter] == value][target].tolist()[0]]\n",
    "        # entropy != 0, and no more spliters, return the highest probablity value\n",
    "        if not features_unused:\n",
    "            return [value, entropy, majority(data[data[spliter] == value][target].tolist())]\n",
    "        # keep splitting\n",
    "        return [value, entropy, get_tree(data[data[spliter] == value], features_unused)]\n",
    "    \n",
    "    def majority(data):\n",
    "        count = dict()\n",
    "        for v in data:\n",
    "            if v not in count:\n",
    "                count[v] = 1\n",
    "            else:\n",
    "                count[v] += 1\n",
    "        return max(count, key=count.get)\n",
    "    \n",
    "    tree = get_tree(training_data, features)\n",
    "    ic(tree)\n",
    "    \n",
    "    def predict_func(data):\n",
    "        # search the tree until find the target value\n",
    "        def next_node(data, tree):\n",
    "            node = tree[0]\n",
    "            for sub_tree in tree[1:]:\n",
    "                if data[node] == sub_tree[0]:\n",
    "                    if isinstance(sub_tree[2], int):\n",
    "                        return sub_tree[2]\n",
    "                    else:\n",
    "                        return next_node(data, sub_tree[2])\n",
    "        return next_node(data, tree)\n",
    "    return predict_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mock data in class\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tree: ['family_number',\n",
      "           [1,\n",
      "            0.6730116670092565,\n",
      "            ['income',\n",
      "             ['+10',\n",
      "              0.5623351446188083,\n",
      "              ['gender', ['M', -0.0, 0], ['F', 0.6931471805599453, 1]]],\n",
      "             ['-10', -0.0, 1]]],\n",
      "           [2, -0.0, 1]]\n"
     ]
    }
   ],
   "source": [
    "model = decision_tree(dataset, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model({'gender': 'M', 'income': '-10', 'family_number': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.decision_tree.<locals>.predict_func(data)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model({'gender': 'F', 'income': '+10', 'family_number': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model({'gender': 'M', 'income': '+10', 'family_number': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标函数\n",
    "$$ y = k*rm + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target function  \n",
    "def price(rm, k, b):\n",
    "    return k * rm + b   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数$$ loss = \\frac{1}{n} \\sum{|y_i - \\hat{y_i}|}$$\n",
    "        $$ loss = \\frac{1}{n} \\sum{|y_i - (kx_i + b_i)|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function \n",
    "def loss(y,y_hat):  # y_hat为预测的y\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))  #zip()打包为元组的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston() #载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=dataset['data'],dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义偏导数\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = \n",
    "             \\begin{cases}\n",
    "             -\\frac{1}{n}\\sum x_i ,\\, y_i \\geq  \\hat{y_i} \\\\\n",
    "             \\frac{1}{n}\\sum x_i ,\\, y_i < \\hat{y_i} \\\\\n",
    "             \\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = \n",
    "             \\begin{cases}\n",
    "             -\\frac{1}{n},\\, y_i \\geq  \\hat{y_i} \\\\\n",
    "             \\frac{1}{n},\\, y_i < \\hat{y_i} \\\\\n",
    "             \\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if y_i >=  y_hat_i:\n",
    "            gradient += -x_i\n",
    "        else:\n",
    "            gradient += x_i\n",
    "    return 1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        if y_i >=  y_hat_i:\n",
    "            return -1 / n \n",
    "        else:\n",
    "            return 1 / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 313.8621691131462, parameters k is -37.08914908861054 and b is -58.237621029136896\n",
      "Iteration 1, the loss is 313.4671830564736, parameters k is -37.02630274473702 and b is -58.237601266291044\n",
      "Iteration 2, the loss is 313.07219699980044, parameters k is -36.96345640086351 and b is -58.23758150344519\n",
      "Iteration 3, the loss is 312.677210943128, parameters k is -36.90061005698999 and b is -58.23756174059934\n",
      "Iteration 4, the loss is 312.28222488645537, parameters k is -36.83776371311647 and b is -58.23754197775349\n",
      "Iteration 5, the loss is 311.88723882978235, parameters k is -36.77491736924296 and b is -58.237522214907635\n",
      "Iteration 6, the loss is 311.4922527731098, parameters k is -36.71207102536944 and b is -58.23750245206178\n",
      "Iteration 7, the loss is 311.0972667164372, parameters k is -36.64922468149592 and b is -58.23748268921593\n",
      "Iteration 8, the loss is 310.7022806597647, parameters k is -36.586378337622406 and b is -58.23746292637008\n",
      "Iteration 9, the loss is 310.3072946030921, parameters k is -36.52353199374889 and b is -58.237443163524226\n",
      "Iteration 10, the loss is 309.9123085464194, parameters k is -36.46068564987537 and b is -58.237423400678374\n",
      "Iteration 11, the loss is 309.51732248974673, parameters k is -36.397839306001856 and b is -58.23740363783252\n",
      "Iteration 12, the loss is 309.12233643307405, parameters k is -36.33499296212834 and b is -58.23738387498667\n",
      "Iteration 13, the loss is 308.72735037640166, parameters k is -36.27214661825482 and b is -58.23736411214082\n",
      "Iteration 14, the loss is 308.33236431972864, parameters k is -36.209300274381306 and b is -58.237344349294965\n",
      "Iteration 15, the loss is 307.9373782630559, parameters k is -36.14645393050779 and b is -58.23732458644911\n",
      "Iteration 16, the loss is 307.54239220638334, parameters k is -36.08360758663427 and b is -58.23730482360326\n",
      "Iteration 17, the loss is 307.1474061497106, parameters k is -36.020761242760756 and b is -58.23728506075741\n",
      "Iteration 18, the loss is 306.7524200930378, parameters k is -35.95791489888724 and b is -58.237265297911556\n",
      "Iteration 19, the loss is 306.35743403636525, parameters k is -35.89506855501372 and b is -58.2372455350657\n",
      "Iteration 20, the loss is 305.96244797969234, parameters k is -35.832222211140206 and b is -58.23722577221985\n",
      "Iteration 21, the loss is 305.56746192301955, parameters k is -35.76937586726669 and b is -58.237206009374\n",
      "Iteration 22, the loss is 305.1724758663467, parameters k is -35.70652952339317 and b is -58.23718624652815\n",
      "Iteration 23, the loss is 304.77748980967425, parameters k is -35.643683179519655 and b is -58.237166483682294\n",
      "Iteration 24, the loss is 304.38250375300186, parameters k is -35.58083683564614 and b is -58.23714672083644\n",
      "Iteration 25, the loss is 303.987517696329, parameters k is -35.51799049177262 and b is -58.23712695799059\n",
      "Iteration 26, the loss is 303.5925316396561, parameters k is -35.455144147899105 and b is -58.23710719514474\n",
      "Iteration 27, the loss is 303.1975455829835, parameters k is -35.39229780402559 and b is -58.237087432298885\n",
      "Iteration 28, the loss is 302.80255952631086, parameters k is -35.32945146015207 and b is -58.23706766945303\n",
      "Iteration 29, the loss is 302.4075734696384, parameters k is -35.266605116278555 and b is -58.23704790660718\n",
      "Iteration 30, the loss is 302.01258741296533, parameters k is -35.20375877240504 and b is -58.23702814376133\n",
      "Iteration 31, the loss is 301.61760135629305, parameters k is -35.14091242853152 and b is -58.237008380915476\n",
      "Iteration 32, the loss is 301.2226152996201, parameters k is -35.078066084658005 and b is -58.236988618069624\n",
      "Iteration 33, the loss is 300.82762924294735, parameters k is -35.01521974078449 and b is -58.23696885522377\n",
      "Iteration 34, the loss is 300.4326431862748, parameters k is -34.95237339691097 and b is -58.23694909237792\n",
      "Iteration 35, the loss is 300.0376571296017, parameters k is -34.889527053037455 and b is -58.23692932953207\n",
      "Iteration 36, the loss is 299.6426710729296, parameters k is -34.82668070916394 and b is -58.236909566686215\n",
      "Iteration 37, the loss is 299.24768501625675, parameters k is -34.76383436529042 and b is -58.23688980384036\n",
      "Iteration 38, the loss is 298.8526989595843, parameters k is -34.700988021416904 and b is -58.23687004099451\n",
      "Iteration 39, the loss is 298.4577129029113, parameters k is -34.63814167754339 and b is -58.23685027814866\n",
      "Iteration 40, the loss is 298.06272684623843, parameters k is -34.57529533366987 and b is -58.236830515302806\n",
      "Iteration 41, the loss is 297.6677407895659, parameters k is -34.512448989796354 and b is -58.236810752456954\n",
      "Iteration 42, the loss is 297.272754732893, parameters k is -34.44960264592284 and b is -58.2367909896111\n",
      "Iteration 43, the loss is 296.8777686762207, parameters k is -34.38675630204932 and b is -58.23677122676525\n",
      "Iteration 44, the loss is 296.4827826195477, parameters k is -34.323909958175804 and b is -58.2367514639194\n",
      "Iteration 45, the loss is 296.08779656287487, parameters k is -34.26106361430229 and b is -58.236731701073545\n",
      "Iteration 46, the loss is 295.6928105062025, parameters k is -34.19821727042877 and b is -58.23671193822769\n",
      "Iteration 47, the loss is 295.2978244495296, parameters k is -34.135370926555254 and b is -58.23669217538184\n",
      "Iteration 48, the loss is 294.90283839285655, parameters k is -34.07252458268174 and b is -58.23667241253599\n",
      "Iteration 49, the loss is 294.5078523361847, parameters k is -34.00967823880822 and b is -58.236652649690136\n",
      "Iteration 50, the loss is 294.1128662795117, parameters k is -33.946831894934704 and b is -58.23663288684428\n",
      "Iteration 51, the loss is 293.71788022283914, parameters k is -33.88398555106119 and b is -58.23661312399843\n",
      "Iteration 52, the loss is 293.32289416616635, parameters k is -33.82113920718767 and b is -58.23659336115258\n",
      "Iteration 53, the loss is 292.9279081094939, parameters k is -33.75829286331415 and b is -58.23657359830673\n",
      "Iteration 54, the loss is 292.53292205282094, parameters k is -33.69544651944064 and b is -58.236553835460874\n",
      "Iteration 55, the loss is 292.1379359961483, parameters k is -33.63260017556712 and b is -58.23653407261502\n",
      "Iteration 56, the loss is 291.74294993947535, parameters k is -33.5697538316936 and b is -58.23651430976917\n",
      "Iteration 57, the loss is 291.3479638828029, parameters k is -33.50690748782009 and b is -58.23649454692332\n",
      "Iteration 58, the loss is 290.9529778261301, parameters k is -33.44406114394657 and b is -58.236474784077465\n",
      "Iteration 59, the loss is 290.5579917694573, parameters k is -33.38121480007305 and b is -58.23645502123161\n",
      "Iteration 60, the loss is 290.16300571278464, parameters k is -33.318368456199536 and b is -58.23643525838576\n",
      "Iteration 61, the loss is 289.7680196561122, parameters k is -33.25552211232602 and b is -58.23641549553991\n",
      "Iteration 62, the loss is 289.37303359943917, parameters k is -33.1926757684525 and b is -58.23639573269406\n",
      "Iteration 63, the loss is 288.97804754276655, parameters k is -33.129829424578986 and b is -58.236375969848204\n",
      "Iteration 64, the loss is 288.58306148609455, parameters k is -33.06698308070547 and b is -58.23635620700235\n",
      "Iteration 65, the loss is 288.18807542942164, parameters k is -33.00413673683195 and b is -58.2363364441565\n",
      "Iteration 66, the loss is 287.79308937274897, parameters k is -32.941290392958436 and b is -58.23631668131065\n",
      "Iteration 67, the loss is 287.398103316076, parameters k is -32.87844404908492 and b is -58.236296918464795\n",
      "Iteration 68, the loss is 287.00311725940344, parameters k is -32.8155977052114 and b is -58.23627715561894\n",
      "Iteration 69, the loss is 286.60813120273093, parameters k is -32.752751361337886 and b is -58.23625739277309\n",
      "Iteration 70, the loss is 286.2131451460576, parameters k is -32.68990501746437 and b is -58.23623762992724\n",
      "Iteration 71, the loss is 285.81815908938535, parameters k is -32.62705867359085 and b is -58.236217867081386\n",
      "Iteration 72, the loss is 285.42317303271244, parameters k is -32.564212329717336 and b is -58.236198104235534\n",
      "Iteration 73, the loss is 285.02818697604, parameters k is -32.50136598584382 and b is -58.23617834138968\n",
      "Iteration 74, the loss is 284.63320091936725, parameters k is -32.4385196419703 and b is -58.23615857854383\n",
      "Iteration 75, the loss is 284.23821486269435, parameters k is -32.375673298096785 and b is -58.23613881569798\n",
      "Iteration 76, the loss is 283.8432288060221, parameters k is -32.31282695422327 and b is -58.236119052852125\n",
      "Iteration 77, the loss is 283.44824274934905, parameters k is -32.24998061034975 and b is -58.23609929000627\n",
      "Iteration 78, the loss is 283.0532566926763, parameters k is -32.187134266476235 and b is -58.23607952716042\n",
      "Iteration 79, the loss is 282.6582706360037, parameters k is -32.12428792260272 and b is -58.23605976431457\n",
      "Iteration 80, the loss is 282.2632845793312, parameters k is -32.0614415787292 and b is -58.236040001468716\n",
      "Iteration 81, the loss is 281.8682985226582, parameters k is -31.998595234855685 and b is -58.236020238622864\n",
      "Iteration 82, the loss is 281.4733124659857, parameters k is -31.93574889098217 and b is -58.23600047577701\n",
      "Iteration 83, the loss is 281.078326409313, parameters k is -31.87290254710865 and b is -58.23598071293116\n",
      "Iteration 84, the loss is 280.68334035264047, parameters k is -31.810056203235135 and b is -58.23596095008531\n",
      "Iteration 85, the loss is 280.28835429596774, parameters k is -31.747209859361618 and b is -58.235941187239455\n",
      "Iteration 86, the loss is 279.89336823929466, parameters k is -31.6843635154881 and b is -58.2359214243936\n",
      "Iteration 87, the loss is 279.49838218262204, parameters k is -31.621517171614585 and b is -58.23590166154775\n",
      "Iteration 88, the loss is 279.1033961259496, parameters k is -31.558670827741068 and b is -58.2358818987019\n",
      "Iteration 89, the loss is 278.7084100692768, parameters k is -31.49582448386755 and b is -58.235862135856046\n",
      "Iteration 90, the loss is 278.3134240126038, parameters k is -31.432978139994034 and b is -58.23584237301019\n",
      "Iteration 91, the loss is 277.9184379559316, parameters k is -31.370131796120518 and b is -58.23582261016434\n",
      "Iteration 92, the loss is 277.52345189925876, parameters k is -31.307285452247 and b is -58.23580284731849\n",
      "Iteration 93, the loss is 277.1284658425862, parameters k is -31.244439108373484 and b is -58.23578308447264\n",
      "Iteration 94, the loss is 276.7334797859132, parameters k is -31.181592764499968 and b is -58.235763321626784\n",
      "Iteration 95, the loss is 276.33849372924084, parameters k is -31.11874642062645 and b is -58.23574355878093\n",
      "Iteration 96, the loss is 275.94350767256805, parameters k is -31.055900076752934 and b is -58.23572379593508\n",
      "Iteration 97, the loss is 275.54852161589537, parameters k is -30.993053732879417 and b is -58.23570403308923\n",
      "Iteration 98, the loss is 275.1535355592225, parameters k is -30.9302073890059 and b is -58.235684270243375\n",
      "Iteration 99, the loss is 274.7585495025496, parameters k is -30.867361045132384 and b is -58.23566450739752\n",
      "Iteration 100, the loss is 274.36356344587733, parameters k is -30.804514701258867 and b is -58.23564474455167\n",
      "Iteration 101, the loss is 273.9685773892045, parameters k is -30.74166835738535 and b is -58.23562498170582\n",
      "Iteration 102, the loss is 273.57359133253163, parameters k is -30.678822013511834 and b is -58.235605218859966\n",
      "Iteration 103, the loss is 273.1786052758593, parameters k is -30.615975669638317 and b is -58.235585456014114\n",
      "Iteration 104, the loss is 272.78361921918656, parameters k is -30.5531293257648 and b is -58.23556569316826\n",
      "Iteration 105, the loss is 272.38863316251394, parameters k is -30.490282981891283 and b is -58.23554593032241\n",
      "Iteration 106, the loss is 271.9936471058409, parameters k is -30.427436638017767 and b is -58.23552616747656\n",
      "Iteration 107, the loss is 271.5986610491684, parameters k is -30.36459029414425 and b is -58.235506404630705\n",
      "Iteration 108, the loss is 271.20367499249545, parameters k is -30.301743950270733 and b is -58.23548664178485\n",
      "Iteration 109, the loss is 270.8086889358229, parameters k is -30.238897606397217 and b is -58.235466878939\n",
      "Iteration 110, the loss is 270.41370287915026, parameters k is -30.1760512625237 and b is -58.23544711609315\n",
      "Iteration 111, the loss is 270.01871682247787, parameters k is -30.113204918650183 and b is -58.235427353247296\n",
      "Iteration 112, the loss is 269.62373076580496, parameters k is -30.050358574776666 and b is -58.235407590401444\n",
      "Iteration 113, the loss is 269.2287447091322, parameters k is -29.98751223090315 and b is -58.23538782755559\n",
      "Iteration 114, the loss is 268.8337586524595, parameters k is -29.924665887029633 and b is -58.23536806470974\n",
      "Iteration 115, the loss is 268.43877259578704, parameters k is -29.861819543156116 and b is -58.23534830186389\n",
      "Iteration 116, the loss is 268.0437865391142, parameters k is -29.7989731992826 and b is -58.235328539018035\n",
      "Iteration 117, the loss is 267.6488004824415, parameters k is -29.736126855409083 and b is -58.23530877617218\n",
      "Iteration 118, the loss is 267.2538144257686, parameters k is -29.673280511535566 and b is -58.23528901332633\n",
      "Iteration 119, the loss is 266.8588283690964, parameters k is -29.61043416766205 and b is -58.23526925048048\n",
      "Iteration 120, the loss is 266.46384231242354, parameters k is -29.547587823788533 and b is -58.235249487634626\n",
      "Iteration 121, the loss is 266.0688562557507, parameters k is -29.484741479915016 and b is -58.23522972478877\n",
      "Iteration 122, the loss is 265.6738701990782, parameters k is -29.4218951360415 and b is -58.23520996194292\n",
      "Iteration 123, the loss is 265.2788841424053, parameters k is -29.359048792167982 and b is -58.23519019909707\n",
      "Iteration 124, the loss is 264.88389808573294, parameters k is -29.296202448294466 and b is -58.23517043625122\n",
      "Iteration 125, the loss is 264.48891202906, parameters k is -29.23335610442095 and b is -58.235150673405364\n",
      "Iteration 126, the loss is 264.09392597238724, parameters k is -29.170509760547432 and b is -58.23513091055951\n",
      "Iteration 127, the loss is 263.69893991571456, parameters k is -29.107663416673915 and b is -58.23511114771366\n",
      "Iteration 128, the loss is 263.303953859042, parameters k is -29.0448170728004 and b is -58.23509138486781\n",
      "Iteration 129, the loss is 262.9089678023692, parameters k is -28.981970728926882 and b is -58.235071622021955\n",
      "Iteration 130, the loss is 262.5139817456963, parameters k is -28.919124385053365 and b is -58.2350518591761\n",
      "Iteration 131, the loss is 262.11899568902373, parameters k is -28.85627804117985 and b is -58.23503209633025\n",
      "Iteration 132, the loss is 261.72400963235094, parameters k is -28.793431697306332 and b is -58.2350123334844\n",
      "Iteration 133, the loss is 261.3290235756782, parameters k is -28.730585353432815 and b is -58.234992570638546\n",
      "Iteration 134, the loss is 260.93403751900576, parameters k is -28.6677390095593 and b is -58.234972807792694\n",
      "Iteration 135, the loss is 260.5390514623328, parameters k is -28.60489266568578 and b is -58.23495304494684\n",
      "Iteration 136, the loss is 260.14406540566057, parameters k is -28.542046321812265 and b is -58.23493328210099\n",
      "Iteration 137, the loss is 259.7490793489874, parameters k is -28.479199977938748 and b is -58.23491351925514\n",
      "Iteration 138, the loss is 259.3540932923151, parameters k is -28.41635363406523 and b is -58.234893756409285\n",
      "Iteration 139, the loss is 258.95910723564225, parameters k is -28.353507290191715 and b is -58.23487399356343\n",
      "Iteration 140, the loss is 258.56412117896974, parameters k is -28.290660946318198 and b is -58.23485423071758\n",
      "Iteration 141, the loss is 258.1691351222969, parameters k is -28.22781460244468 and b is -58.23483446787173\n",
      "Iteration 142, the loss is 257.7741490656242, parameters k is -28.164968258571164 and b is -58.234814705025876\n",
      "Iteration 143, the loss is 257.3791630089517, parameters k is -28.102121914697648 and b is -58.234794942180024\n",
      "Iteration 144, the loss is 256.9841769522791, parameters k is -28.03927557082413 and b is -58.23477517933417\n",
      "Iteration 145, the loss is 256.58919089560595, parameters k is -27.976429226950614 and b is -58.23475541648832\n",
      "Iteration 146, the loss is 256.1942048389334, parameters k is -27.913582883077098 and b is -58.23473565364247\n",
      "Iteration 147, the loss is 255.79921878226082, parameters k is -27.85073653920358 and b is -58.234715890796615\n",
      "Iteration 148, the loss is 255.40423272558797, parameters k is -27.787890195330064 and b is -58.23469612795076\n",
      "Iteration 149, the loss is 255.0092466689152, parameters k is -27.725043851456547 and b is -58.23467636510491\n",
      "Iteration 150, the loss is 254.61426061224265, parameters k is -27.66219750758303 and b is -58.23465660225906\n",
      "Iteration 151, the loss is 254.21927455557014, parameters k is -27.599351163709514 and b is -58.234636839413206\n",
      "Iteration 152, the loss is 253.82428849889737, parameters k is -27.536504819835997 and b is -58.23461707656735\n",
      "Iteration 153, the loss is 253.42930244222467, parameters k is -27.47365847596248 and b is -58.2345973137215\n",
      "Iteration 154, the loss is 253.03431638555185, parameters k is -27.410812132088964 and b is -58.23457755087565\n",
      "Iteration 155, the loss is 252.63933032887908, parameters k is -27.347965788215447 and b is -58.2345577880298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, the loss is 252.24434427220655, parameters k is -27.28511944434193 and b is -58.234538025183944\n",
      "Iteration 157, the loss is 251.84935821553387, parameters k is -27.222273100468414 and b is -58.23451826233809\n",
      "Iteration 158, the loss is 251.45437215886113, parameters k is -27.159426756594897 and b is -58.23449849949224\n",
      "Iteration 159, the loss is 251.05938610218854, parameters k is -27.09658041272138 and b is -58.23447873664639\n",
      "Iteration 160, the loss is 250.66440004551595, parameters k is -27.033734068847863 and b is -58.234458973800535\n",
      "Iteration 161, the loss is 250.26941398884307, parameters k is -26.970887724974347 and b is -58.23443921095468\n",
      "Iteration 162, the loss is 249.87442793217053, parameters k is -26.90804138110083 and b is -58.23441944810883\n",
      "Iteration 163, the loss is 249.47944187549763, parameters k is -26.845195037227313 and b is -58.23439968526298\n",
      "Iteration 164, the loss is 249.08445581882515, parameters k is -26.782348693353796 and b is -58.234379922417126\n",
      "Iteration 165, the loss is 248.68946976215236, parameters k is -26.71950234948028 and b is -58.234360159571274\n",
      "Iteration 166, the loss is 248.29448370547965, parameters k is -26.656656005606763 and b is -58.23434039672542\n",
      "Iteration 167, the loss is 247.89949764880708, parameters k is -26.593809661733246 and b is -58.23432063387957\n",
      "Iteration 168, the loss is 247.50451159213398, parameters k is -26.53096331785973 and b is -58.23430087103372\n",
      "Iteration 169, the loss is 247.10952553546144, parameters k is -26.468116973986213 and b is -58.234281108187865\n",
      "Iteration 170, the loss is 246.7145394787887, parameters k is -26.405270630112696 and b is -58.23426134534201\n",
      "Iteration 171, the loss is 246.31955342211648, parameters k is -26.34242428623918 and b is -58.23424158249616\n",
      "Iteration 172, the loss is 245.92456736544335, parameters k is -26.279577942365663 and b is -58.23422181965031\n",
      "Iteration 173, the loss is 245.5295813087708, parameters k is -26.216731598492146 and b is -58.234202056804456\n",
      "Iteration 174, the loss is 245.134595252098, parameters k is -26.15388525461863 and b is -58.234182293958604\n",
      "Iteration 175, the loss is 244.73960919542552, parameters k is -26.091038910745112 and b is -58.23416253111275\n",
      "Iteration 176, the loss is 244.34462313875287, parameters k is -26.028192566871596 and b is -58.2341427682669\n",
      "Iteration 177, the loss is 243.94963708207993, parameters k is -25.96534622299808 and b is -58.23412300542105\n",
      "Iteration 178, the loss is 243.55465102540722, parameters k is -25.902499879124562 and b is -58.234103242575195\n",
      "Iteration 179, the loss is 243.15966496873477, parameters k is -25.839653535251045 and b is -58.23408347972934\n",
      "Iteration 180, the loss is 242.76467891206204, parameters k is -25.77680719137753 and b is -58.23406371688349\n",
      "Iteration 181, the loss is 242.36969285538933, parameters k is -25.713960847504012 and b is -58.23404395403764\n",
      "Iteration 182, the loss is 241.97470679871665, parameters k is -25.651114503630495 and b is -58.234024191191786\n",
      "Iteration 183, the loss is 241.5797207420441, parameters k is -25.58826815975698 and b is -58.23400442834593\n",
      "Iteration 184, the loss is 241.18473468537127, parameters k is -25.525421815883462 and b is -58.23398466550008\n",
      "Iteration 185, the loss is 240.78974862869833, parameters k is -25.462575472009945 and b is -58.23396490265423\n",
      "Iteration 186, the loss is 240.39476257202568, parameters k is -25.39972912813643 and b is -58.23394513980838\n",
      "Iteration 187, the loss is 239.99977651535303, parameters k is -25.33688278426291 and b is -58.233925376962524\n",
      "Iteration 188, the loss is 239.60479045868047, parameters k is -25.274036440389395 and b is -58.23390561411667\n",
      "Iteration 189, the loss is 239.20980440200773, parameters k is -25.211190096515878 and b is -58.23388585127082\n",
      "Iteration 190, the loss is 238.814818345335, parameters k is -25.14834375264236 and b is -58.23386608842497\n",
      "Iteration 191, the loss is 238.41983228866255, parameters k is -25.085497408768845 and b is -58.233846325579115\n",
      "Iteration 192, the loss is 238.02484623198953, parameters k is -25.022651064895328 and b is -58.23382656273326\n",
      "Iteration 193, the loss is 237.62986017531722, parameters k is -24.95980472102181 and b is -58.23380679988741\n",
      "Iteration 194, the loss is 237.23487411864426, parameters k is -24.896958377148295 and b is -58.23378703704156\n",
      "Iteration 195, the loss is 236.83988806197178, parameters k is -24.834112033274778 and b is -58.233767274195706\n",
      "Iteration 196, the loss is 236.4449020052991, parameters k is -24.77126568940126 and b is -58.233747511349854\n",
      "Iteration 197, the loss is 236.04991594862622, parameters k is -24.708419345527744 and b is -58.233727748504\n",
      "Iteration 198, the loss is 235.6549298919538, parameters k is -24.645573001654228 and b is -58.23370798565815\n",
      "Iteration 199, the loss is 235.2599438352809, parameters k is -24.58272665778071 and b is -58.2336882228123\n",
      "Iteration 200, the loss is 234.86495777860824, parameters k is -24.519880313907194 and b is -58.233668459966445\n",
      "Iteration 201, the loss is 234.46997172193576, parameters k is -24.457033970033677 and b is -58.23364869712059\n",
      "Iteration 202, the loss is 234.0749856652629, parameters k is -24.39418762616016 and b is -58.23362893427474\n",
      "Iteration 203, the loss is 233.67999960859004, parameters k is -24.331341282286644 and b is -58.23360917142889\n",
      "Iteration 204, the loss is 233.2850135519174, parameters k is -24.268494938413127 and b is -58.233589408583036\n",
      "Iteration 205, the loss is 232.8900274952447, parameters k is -24.20564859453961 and b is -58.233569645737184\n",
      "Iteration 206, the loss is 232.4950414385721, parameters k is -24.142802250666094 and b is -58.23354988289133\n",
      "Iteration 207, the loss is 232.10005538189938, parameters k is -24.079955906792577 and b is -58.23353012004548\n",
      "Iteration 208, the loss is 231.70506932522665, parameters k is -24.01710956291906 and b is -58.23351035719963\n",
      "Iteration 209, the loss is 231.3100832685539, parameters k is -23.954263219045544 and b is -58.233490594353775\n",
      "Iteration 210, the loss is 230.91509721188146, parameters k is -23.891416875172027 and b is -58.23347083150792\n",
      "Iteration 211, the loss is 230.52011115520875, parameters k is -23.82857053129851 and b is -58.23345106866207\n",
      "Iteration 212, the loss is 230.12512509853607, parameters k is -23.765724187424993 and b is -58.23343130581622\n",
      "Iteration 213, the loss is 229.73013904186325, parameters k is -23.702877843551477 and b is -58.233411542970366\n",
      "Iteration 214, the loss is 229.3351529851905, parameters k is -23.64003149967796 and b is -58.233391780124514\n",
      "Iteration 215, the loss is 228.94016692851775, parameters k is -23.577185155804443 and b is -58.23337201727866\n",
      "Iteration 216, the loss is 228.54518087184513, parameters k is -23.514338811930926 and b is -58.23335225443281\n",
      "Iteration 217, the loss is 228.15019481517243, parameters k is -23.45149246805741 and b is -58.23333249158696\n",
      "Iteration 218, the loss is 227.7552087584995, parameters k is -23.388646124183893 and b is -58.233312728741105\n",
      "Iteration 219, the loss is 227.36022270182713, parameters k is -23.325799780310376 and b is -58.23329296589525\n",
      "Iteration 220, the loss is 226.96523664515425, parameters k is -23.26295343643686 and b is -58.2332732030494\n",
      "Iteration 221, the loss is 226.5702505884817, parameters k is -23.200107092563343 and b is -58.23325344020355\n",
      "Iteration 222, the loss is 226.17526453180884, parameters k is -23.137260748689826 and b is -58.233233677357696\n",
      "Iteration 223, the loss is 225.78027847513613, parameters k is -23.07441440481631 and b is -58.23321391451184\n",
      "Iteration 224, the loss is 225.38529241846396, parameters k is -23.011568060942793 and b is -58.23319415166599\n",
      "Iteration 225, the loss is 224.99030636179126, parameters k is -22.948721717069276 and b is -58.23317438882014\n",
      "Iteration 226, the loss is 224.59532030511838, parameters k is -22.88587537319576 and b is -58.23315462597429\n",
      "Iteration 227, the loss is 224.20033424844567, parameters k is -22.823029029322242 and b is -58.233134863128434\n",
      "Iteration 228, the loss is 223.80534819177282, parameters k is -22.760182685448726 and b is -58.23311510028258\n",
      "Iteration 229, the loss is 223.41036213510017, parameters k is -22.69733634157521 and b is -58.23309533743673\n",
      "Iteration 230, the loss is 223.01537607842747, parameters k is -22.634489997701692 and b is -58.23307557459088\n",
      "Iteration 231, the loss is 222.62039002175487, parameters k is -22.571643653828176 and b is -58.233055811745025\n",
      "Iteration 232, the loss is 222.22540396508202, parameters k is -22.50879730995466 and b is -58.23303604889917\n",
      "Iteration 233, the loss is 221.83041790840926, parameters k is -22.445950966081142 and b is -58.23301628605332\n",
      "Iteration 234, the loss is 221.43543185173678, parameters k is -22.383104622207625 and b is -58.23299652320747\n",
      "Iteration 235, the loss is 221.04044579506407, parameters k is -22.32025827833411 and b is -58.232976760361616\n",
      "Iteration 236, the loss is 220.64545973839134, parameters k is -22.257411934460592 and b is -58.232956997515764\n",
      "Iteration 237, the loss is 220.25047368171857, parameters k is -22.194565590587075 and b is -58.23293723466991\n",
      "Iteration 238, the loss is 219.85548762504612, parameters k is -22.13171924671356 and b is -58.23291747182406\n",
      "Iteration 239, the loss is 219.46050156837333, parameters k is -22.06887290284004 and b is -58.23289770897821\n",
      "Iteration 240, the loss is 219.0655155117007, parameters k is -22.006026558966525 and b is -58.232877946132355\n",
      "Iteration 241, the loss is 218.67052945502786, parameters k is -21.943180215093008 and b is -58.2328581832865\n",
      "Iteration 242, the loss is 218.27554339835524, parameters k is -21.88033387121949 and b is -58.23283842044065\n",
      "Iteration 243, the loss is 217.88055734168253, parameters k is -21.817487527345975 and b is -58.2328186575948\n",
      "Iteration 244, the loss is 217.48557128500968, parameters k is -21.754641183472458 and b is -58.232798894748946\n",
      "Iteration 245, the loss is 217.0905852283371, parameters k is -21.69179483959894 and b is -58.232779131903094\n",
      "Iteration 246, the loss is 216.69559917166438, parameters k is -21.628948495725425 and b is -58.23275936905724\n",
      "Iteration 247, the loss is 216.3006131149918, parameters k is -21.566102151851908 and b is -58.23273960621139\n",
      "Iteration 248, the loss is 215.90562705831883, parameters k is -21.50325580797839 and b is -58.23271984336554\n",
      "Iteration 249, the loss is 215.51064100164655, parameters k is -21.440409464104874 and b is -58.232700080519685\n",
      "Iteration 250, the loss is 215.11565494497356, parameters k is -21.377563120231358 and b is -58.23268031767383\n",
      "Iteration 251, the loss is 214.72066888830102, parameters k is -21.31471677635784 and b is -58.23266055482798\n",
      "Iteration 252, the loss is 214.32568283162817, parameters k is -21.251870432484324 and b is -58.23264079198213\n",
      "Iteration 253, the loss is 213.93069677495572, parameters k is -21.189024088610807 and b is -58.232621029136276\n",
      "Iteration 254, the loss is 213.5357107182832, parameters k is -21.12617774473729 and b is -58.23260126629042\n",
      "Iteration 255, the loss is 213.14072466161025, parameters k is -21.063331400863774 and b is -58.23258150344457\n",
      "Iteration 256, the loss is 212.74573860493766, parameters k is -21.000485056990257 and b is -58.23256174059872\n",
      "Iteration 257, the loss is 212.35075254826475, parameters k is -20.93763871311674 and b is -58.23254197775287\n",
      "Iteration 258, the loss is 211.95576649159187, parameters k is -20.874792369243224 and b is -58.232522214907014\n",
      "Iteration 259, the loss is 211.56078043491962, parameters k is -20.811946025369707 and b is -58.23250245206116\n",
      "Iteration 260, the loss is 211.16579437824637, parameters k is -20.74909968149619 and b is -58.23248268921531\n",
      "Iteration 261, the loss is 210.77080832157418, parameters k is -20.686253337622674 and b is -58.23246292636946\n",
      "Iteration 262, the loss is 210.3758222649015, parameters k is -20.623406993749157 and b is -58.232443163523605\n",
      "Iteration 263, the loss is 209.98083620822865, parameters k is -20.56056064987564 and b is -58.23242340067775\n",
      "Iteration 264, the loss is 209.58585015155603, parameters k is -20.497714306002123 and b is -58.2324036378319\n",
      "Iteration 265, the loss is 209.19086409488338, parameters k is -20.434867962128607 and b is -58.23238387498605\n",
      "Iteration 266, the loss is 208.79587803821047, parameters k is -20.37202161825509 and b is -58.232364112140196\n",
      "Iteration 267, the loss is 208.40089198153774, parameters k is -20.309175274381573 and b is -58.232344349294344\n",
      "Iteration 268, the loss is 208.0059059248652, parameters k is -20.246328930508056 and b is -58.23232458644849\n",
      "Iteration 269, the loss is 207.6109198681927, parameters k is -20.18348258663454 and b is -58.23230482360264\n",
      "Iteration 270, the loss is 207.2159338115199, parameters k is -20.120636242761023 and b is -58.23228506075679\n",
      "Iteration 271, the loss is 206.8209477548472, parameters k is -20.057789898887506 and b is -58.232265297910935\n",
      "Iteration 272, the loss is 206.42596169817415, parameters k is -19.99494355501399 and b is -58.23224553506508\n",
      "Iteration 273, the loss is 206.03097564150173, parameters k is -19.932097211140473 and b is -58.23222577221923\n",
      "Iteration 274, the loss is 205.63598958482908, parameters k is -19.869250867266956 and b is -58.23220600937338\n",
      "Iteration 275, the loss is 205.2410035281565, parameters k is -19.80640452339344 and b is -58.232186246527526\n",
      "Iteration 276, the loss is 204.84601747148378, parameters k is -19.743558179519923 and b is -58.232166483681674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 277, the loss is 204.45103141481098, parameters k is -19.680711835646406 and b is -58.23214672083582\n",
      "Iteration 278, the loss is 204.0560453581382, parameters k is -19.61786549177289 and b is -58.23212695798997\n",
      "Iteration 279, the loss is 203.6610593014654, parameters k is -19.555019147899372 and b is -58.23210719514412\n",
      "Iteration 280, the loss is 203.26607324479303, parameters k is -19.492172804025856 and b is -58.232087432298265\n",
      "Iteration 281, the loss is 202.87108718812019, parameters k is -19.42932646015234 and b is -58.23206766945241\n",
      "Iteration 282, the loss is 202.47610113144722, parameters k is -19.366480116278822 and b is -58.23204790660656\n",
      "Iteration 283, the loss is 202.08111507477486, parameters k is -19.303633772405306 and b is -58.23202814376071\n",
      "Iteration 284, the loss is 201.68612901810215, parameters k is -19.24078742853179 and b is -58.232008380914856\n",
      "Iteration 285, the loss is 201.29114296142956, parameters k is -19.177941084658272 and b is -58.231988618069\n",
      "Iteration 286, the loss is 200.89615690475685, parameters k is -19.115094740784755 and b is -58.23196885522315\n",
      "Iteration 287, the loss is 200.50117084808423, parameters k is -19.05224839691124 and b is -58.2319490923773\n",
      "Iteration 288, the loss is 200.10618479141147, parameters k is -18.989402053037722 and b is -58.23192932953145\n",
      "Iteration 289, the loss is 199.7111987347386, parameters k is -18.926555709164205 and b is -58.231909566685594\n",
      "Iteration 290, the loss is 199.31621267806608, parameters k is -18.86370936529069 and b is -58.23188980383974\n",
      "Iteration 291, the loss is 198.92122662139317, parameters k is -18.80086302141717 and b is -58.23187004099389\n",
      "Iteration 292, the loss is 198.52624056472075, parameters k is -18.738016677543655 and b is -58.23185027814804\n",
      "Iteration 293, the loss is 198.13125450804802, parameters k is -18.67517033367014 and b is -58.231830515302185\n",
      "Iteration 294, the loss is 197.7362684513753, parameters k is -18.61232398979662 and b is -58.23181075245633\n",
      "Iteration 295, the loss is 197.34128239470257, parameters k is -18.549477645923105 and b is -58.23179098961048\n",
      "Iteration 296, the loss is 196.94629633803, parameters k is -18.486631302049588 and b is -58.23177122676463\n",
      "Iteration 297, the loss is 196.55131028135716, parameters k is -18.42378495817607 and b is -58.231751463918776\n",
      "Iteration 298, the loss is 196.15632422468454, parameters k is -18.360938614302555 and b is -58.231731701072924\n",
      "Iteration 299, the loss is 195.76133816801163, parameters k is -18.298092270429038 and b is -58.23171193822707\n",
      "Iteration 300, the loss is 195.3663521113392, parameters k is -18.23524592655552 and b is -58.23169217538122\n",
      "Iteration 301, the loss is 194.9713660546665, parameters k is -18.172399582682004 and b is -58.23167241253537\n",
      "Iteration 302, the loss is 194.57637999799385, parameters k is -18.109553238808488 and b is -58.231652649689515\n",
      "Iteration 303, the loss is 194.18139394132115, parameters k is -18.04670689493497 and b is -58.23163288684366\n",
      "Iteration 304, the loss is 193.78640788464844, parameters k is -17.983860551061454 and b is -58.23161312399781\n",
      "Iteration 305, the loss is 193.39142182797548, parameters k is -17.921014207187937 and b is -58.23159336115196\n",
      "Iteration 306, the loss is 192.9964357713031, parameters k is -17.85816786331442 and b is -58.231573598306106\n",
      "Iteration 307, the loss is 192.6014497146305, parameters k is -17.795321519440904 and b is -58.231553835460254\n",
      "Iteration 308, the loss is 192.2064636579577, parameters k is -17.732475175567387 and b is -58.2315340726144\n",
      "Iteration 309, the loss is 191.8114776012849, parameters k is -17.66962883169387 and b is -58.23151430976855\n",
      "Iteration 310, the loss is 191.41649154461246, parameters k is -17.606782487820354 and b is -58.2314945469227\n",
      "Iteration 311, the loss is 191.02150548793966, parameters k is -17.543936143946837 and b is -58.231474784076845\n",
      "Iteration 312, the loss is 190.62651943126696, parameters k is -17.48108980007332 and b is -58.23145502123099\n",
      "Iteration 313, the loss is 190.23153337459416, parameters k is -17.418243456199804 and b is -58.23143525838514\n",
      "Iteration 314, the loss is 189.8365473179215, parameters k is -17.355397112326287 and b is -58.23141549553929\n",
      "Iteration 315, the loss is 189.44156126124867, parameters k is -17.29255076845277 and b is -58.231395732693436\n",
      "Iteration 316, the loss is 189.0465752045763, parameters k is -17.229704424579253 and b is -58.23137596984758\n",
      "Iteration 317, the loss is 188.65158914790356, parameters k is -17.166858080705737 and b is -58.23135620700173\n",
      "Iteration 318, the loss is 188.2566030912307, parameters k is -17.10401173683222 and b is -58.23133644415588\n",
      "Iteration 319, the loss is 187.86161703455787, parameters k is -17.041165392958703 and b is -58.23131668131003\n",
      "Iteration 320, the loss is 187.46663097788556, parameters k is -16.978319049085187 and b is -58.231296918464174\n",
      "Iteration 321, the loss is 187.07164492121268, parameters k is -16.91547270521167 and b is -58.23127715561832\n",
      "Iteration 322, the loss is 186.6766588645401, parameters k is -16.852626361338153 and b is -58.23125739277247\n",
      "Iteration 323, the loss is 186.2816728078671, parameters k is -16.789780017464636 and b is -58.23123762992662\n",
      "Iteration 324, the loss is 185.88668675119456, parameters k is -16.72693367359112 and b is -58.231217867080765\n",
      "Iteration 325, the loss is 185.49170069452197, parameters k is -16.664087329717603 and b is -58.23119810423491\n",
      "Iteration 326, the loss is 185.09671463784926, parameters k is -16.601240985844086 and b is -58.23117834138906\n",
      "Iteration 327, the loss is 184.70172858117633, parameters k is -16.53839464197057 and b is -58.23115857854321\n",
      "Iteration 328, the loss is 184.3067425245039, parameters k is -16.475548298097053 and b is -58.231138815697356\n",
      "Iteration 329, the loss is 183.9117564678309, parameters k is -16.412701954223536 and b is -58.231119052851504\n",
      "Iteration 330, the loss is 183.51677041115843, parameters k is -16.34985561035002 and b is -58.23109929000565\n",
      "Iteration 331, the loss is 183.12178435448567, parameters k is -16.287009266476502 and b is -58.2310795271598\n",
      "Iteration 332, the loss is 182.72679829781302, parameters k is -16.224162922602986 and b is -58.23105976431395\n",
      "Iteration 333, the loss is 182.33181224114009, parameters k is -16.16131657872947 and b is -58.231040001468095\n",
      "Iteration 334, the loss is 181.93682618446772, parameters k is -16.098470234855952 and b is -58.23102023862224\n",
      "Iteration 335, the loss is 181.541840127795, parameters k is -16.035623890982436 and b is -58.23100047577639\n",
      "Iteration 336, the loss is 181.14685407112248, parameters k is -15.972777547108917 and b is -58.23098071293054\n",
      "Iteration 337, the loss is 180.75186801444943, parameters k is -15.909931203235399 and b is -58.230960950084686\n",
      "Iteration 338, the loss is 180.35688195777684, parameters k is -15.84708485936188 and b is -58.230941187238834\n",
      "Iteration 339, the loss is 179.96189590110427, parameters k is -15.784238515488362 and b is -58.23092142439298\n",
      "Iteration 340, the loss is 179.5669098444315, parameters k is -15.721392171614843 and b is -58.23090166154713\n",
      "Iteration 341, the loss is 179.17192378775877, parameters k is -15.658545827741325 and b is -58.23088189870128\n",
      "Iteration 342, the loss is 178.77693773108606, parameters k is -15.595699483867806 and b is -58.230862135855425\n",
      "Iteration 343, the loss is 178.3819516744134, parameters k is -15.532853139994288 and b is -58.23084237300957\n",
      "Iteration 344, the loss is 177.98696561774076, parameters k is -15.470006796120769 and b is -58.23082261016372\n",
      "Iteration 345, the loss is 177.5919795610679, parameters k is -15.40716045224725 and b is -58.23080284731787\n",
      "Iteration 346, the loss is 177.1969935043953, parameters k is -15.344314108373732 and b is -58.230783084472016\n",
      "Iteration 347, the loss is 176.80200744772262, parameters k is -15.281467764500213 and b is -58.23076332162616\n",
      "Iteration 348, the loss is 176.40702139104988, parameters k is -15.218621420626695 and b is -58.23074355878031\n",
      "Iteration 349, the loss is 176.01203533437715, parameters k is -15.155775076753176 and b is -58.23072379593446\n",
      "Iteration 350, the loss is 175.61704927770458, parameters k is -15.092928732879658 and b is -58.23070403308861\n",
      "Iteration 351, the loss is 175.22206322103185, parameters k is -15.03008238900614 and b is -58.230684270242755\n",
      "Iteration 352, the loss is 174.82707716435922, parameters k is -14.967236045132621 and b is -58.2306645073969\n",
      "Iteration 353, the loss is 174.43209110768646, parameters k is -14.904389701259102 and b is -58.23064474455105\n",
      "Iteration 354, the loss is 174.03710505101387, parameters k is -14.841543357385584 and b is -58.2306249817052\n",
      "Iteration 355, the loss is 173.64211899434076, parameters k is -14.778697013512065 and b is -58.230605218859345\n",
      "Iteration 356, the loss is 173.24713293766814, parameters k is -14.715850669638547 and b is -58.23058545601349\n",
      "Iteration 357, the loss is 172.8521468809957, parameters k is -14.653004325765028 and b is -58.23056569316764\n",
      "Iteration 358, the loss is 172.45716082432295, parameters k is -14.59015798189151 and b is -58.23054593032179\n",
      "Iteration 359, the loss is 172.06217476765013, parameters k is -14.527311638017991 and b is -58.23052616747594\n",
      "Iteration 360, the loss is 171.6671887109774, parameters k is -14.464465294144473 and b is -58.230506404630084\n",
      "Iteration 361, the loss is 171.27220265430486, parameters k is -14.401618950270954 and b is -58.23048664178423\n",
      "Iteration 362, the loss is 170.87721659763227, parameters k is -14.338772606397436 and b is -58.23046687893838\n",
      "Iteration 363, the loss is 170.48223054095942, parameters k is -14.275926262523917 and b is -58.23044711609253\n",
      "Iteration 364, the loss is 170.08724448428657, parameters k is -14.213079918650399 and b is -58.230427353246675\n",
      "Iteration 365, the loss is 169.6922584276138, parameters k is -14.15023357477688 and b is -58.23040759040082\n",
      "Iteration 366, the loss is 169.2972723709413, parameters k is -14.087387230903362 and b is -58.23038782755497\n",
      "Iteration 367, the loss is 168.9022863142686, parameters k is -14.024540887029843 and b is -58.23036806470912\n",
      "Iteration 368, the loss is 168.5073002575958, parameters k is -13.961694543156325 and b is -58.230348301863266\n",
      "Iteration 369, the loss is 168.11231420092307, parameters k is -13.898848199282806 and b is -58.230328539017414\n",
      "Iteration 370, the loss is 167.71732814425044, parameters k is -13.836001855409288 and b is -58.23030877617156\n",
      "Iteration 371, the loss is 167.32234208757785, parameters k is -13.77315551153577 and b is -58.23028901332571\n",
      "Iteration 372, the loss is 166.92735603090514, parameters k is -13.71030916766225 and b is -58.23026925047986\n",
      "Iteration 373, the loss is 166.53236997423232, parameters k is -13.647462823788732 and b is -58.230249487634005\n",
      "Iteration 374, the loss is 166.1373839175598, parameters k is -13.584616479915214 and b is -58.23022972478815\n",
      "Iteration 375, the loss is 165.7423978608868, parameters k is -13.521770136041695 and b is -58.2302099619423\n",
      "Iteration 376, the loss is 165.3474118042142, parameters k is -13.458923792168177 and b is -58.23019019909645\n",
      "Iteration 377, the loss is 164.95242574754158, parameters k is -13.396077448294658 and b is -58.230170436250596\n",
      "Iteration 378, the loss is 164.5574396908689, parameters k is -13.33323110442114 and b is -58.230150673404744\n",
      "Iteration 379, the loss is 164.1624536341962, parameters k is -13.270384760547621 and b is -58.23013091055889\n",
      "Iteration 380, the loss is 163.7674675775236, parameters k is -13.207538416674103 and b is -58.23011114771304\n",
      "Iteration 381, the loss is 163.37248152085087, parameters k is -13.144692072800584 and b is -58.23009138486719\n",
      "Iteration 382, the loss is 162.97749546417808, parameters k is -13.081845728927066 and b is -58.230071622021335\n",
      "Iteration 383, the loss is 162.58250940750523, parameters k is -13.018999385053547 and b is -58.23005185917548\n",
      "Iteration 384, the loss is 162.18752335083283, parameters k is -12.956153041180029 and b is -58.23003209632963\n",
      "Iteration 385, the loss is 161.79253729415993, parameters k is -12.89330669730651 and b is -58.23001233348378\n",
      "Iteration 386, the loss is 161.39755123748728, parameters k is -12.830460353432992 and b is -58.229992570637926\n",
      "Iteration 387, the loss is 161.00256518081454, parameters k is -12.767614009559473 and b is -58.22997280779207\n",
      "Iteration 388, the loss is 160.60757912414178, parameters k is -12.704767665685955 and b is -58.22995304494622\n",
      "Iteration 389, the loss is 160.21259306746927, parameters k is -12.641921321812436 and b is -58.22993328210037\n",
      "Iteration 390, the loss is 159.81760701079645, parameters k is -12.579074977938918 and b is -58.22991351925452\n",
      "Iteration 391, the loss is 159.4226209541236, parameters k is -12.5162286340654 and b is -58.229893756408664\n",
      "Iteration 392, the loss is 159.02763489745115, parameters k is -12.45338229019188 and b is -58.22987399356281\n",
      "Iteration 393, the loss is 158.6326488407784, parameters k is -12.390535946318362 and b is -58.22985423071696\n",
      "Iteration 394, the loss is 158.23766278410574, parameters k is -12.327689602444844 and b is -58.22983446787111\n",
      "Iteration 395, the loss is 157.84267672743297, parameters k is -12.264843258571325 and b is -58.229814705025255\n",
      "Iteration 396, the loss is 157.44769067076, parameters k is -12.201996914697807 and b is -58.2297949421794\n",
      "Iteration 397, the loss is 157.05270461408762, parameters k is -12.139150570824288 and b is -58.22977517933355\n",
      "Iteration 398, the loss is 156.65771855741488, parameters k is -12.07630422695077 and b is -58.2297554164877\n",
      "Iteration 399, the loss is 156.26273250074212, parameters k is -12.013457883077251 and b is -58.229735653641846\n",
      "Iteration 400, the loss is 155.86774644406935, parameters k is -11.950611539203733 and b is -58.229715890795994\n",
      "Iteration 401, the loss is 155.4727603873966, parameters k is -11.887765195330214 and b is -58.22969612795014\n",
      "Iteration 402, the loss is 155.07777433072408, parameters k is -11.824918851456696 and b is -58.22967636510429\n",
      "Iteration 403, the loss is 154.68278827405135, parameters k is -11.762072507583177 and b is -58.22965660225844\n",
      "Iteration 404, the loss is 154.28780221737856, parameters k is -11.699226163709659 and b is -58.229636839412585\n",
      "Iteration 405, the loss is 153.89281616070582, parameters k is -11.63637981983614 and b is -58.22961707656673\n",
      "Iteration 406, the loss is 153.4978301040333, parameters k is -11.573533475962622 and b is -58.22959731372088\n",
      "Iteration 407, the loss is 153.10284404736063, parameters k is -11.510687132089103 and b is -58.22957755087503\n",
      "Iteration 408, the loss is 152.7078579906879, parameters k is -11.447840788215585 and b is -58.229557788029176\n",
      "Iteration 409, the loss is 152.31287193401513, parameters k is -11.384994444342066 and b is -58.229538025183324\n",
      "Iteration 410, the loss is 151.91788587734248, parameters k is -11.322148100468548 and b is -58.22951826233747\n",
      "Iteration 411, the loss is 151.5228998206698, parameters k is -11.259301756595029 and b is -58.22949849949162\n",
      "Iteration 412, the loss is 151.12791376399701, parameters k is -11.19645541272151 and b is -58.22947873664577\n",
      "Iteration 413, the loss is 150.73292770732436, parameters k is -11.133609068847992 and b is -58.229458973799915\n",
      "Iteration 414, the loss is 150.33794165065166, parameters k is -11.070762724974474 and b is -58.22943921095406\n",
      "Iteration 415, the loss is 149.94295559397895, parameters k is -11.007916381100955 and b is -58.22941944810821\n",
      "Iteration 416, the loss is 149.5479695373062, parameters k is -10.945070037227437 and b is -58.22939968526236\n",
      "Iteration 417, the loss is 149.15298348063345, parameters k is -10.882223693353918 and b is -58.229379922416506\n",
      "Iteration 418, the loss is 148.75799742396077, parameters k is -10.8193773494804 and b is -58.22936015957065\n",
      "Iteration 419, the loss is 148.36301136728815, parameters k is -10.756531005606881 and b is -58.2293403967248\n",
      "Iteration 420, the loss is 147.96802531061533, parameters k is -10.693684661733363 and b is -58.22932063387895\n",
      "Iteration 421, the loss is 147.57303925394282, parameters k is -10.630838317859844 and b is -58.2293008710331\n",
      "Iteration 422, the loss is 147.1780531972699, parameters k is -10.567991973986326 and b is -58.229281108187244\n",
      "Iteration 423, the loss is 146.78306714059724, parameters k is -10.505145630112807 and b is -58.22926134534139\n",
      "Iteration 424, the loss is 146.3880810839246, parameters k is -10.442299286239288 and b is -58.22924158249554\n",
      "Iteration 425, the loss is 145.99309502725183, parameters k is -10.37945294236577 and b is -58.22922181964969\n",
      "Iteration 426, the loss is 145.5981089705791, parameters k is -10.316606598492251 and b is -58.229202056803835\n",
      "Iteration 427, the loss is 145.20312291390644, parameters k is -10.253760254618733 and b is -58.22918229395798\n",
      "Iteration 428, the loss is 144.80813685723388, parameters k is -10.190913910745214 and b is -58.22916253111213\n",
      "Iteration 429, the loss is 144.41315080056103, parameters k is -10.128067566871696 and b is -58.22914276826628\n",
      "Iteration 430, the loss is 144.0181647438882, parameters k is -10.065221222998177 and b is -58.229123005420426\n",
      "Iteration 431, the loss is 143.62317868721578, parameters k is -10.002374879124659 and b is -58.229103242574574\n",
      "Iteration 432, the loss is 143.22819263054302, parameters k is -9.93952853525114 and b is -58.22908347972872\n",
      "Iteration 433, the loss is 142.83320657387023, parameters k is -9.876682191377622 and b is -58.22906371688287\n",
      "Iteration 434, the loss is 142.43822051719755, parameters k is -9.813835847504103 and b is -58.22904395403702\n",
      "Iteration 435, the loss is 142.04323446052504, parameters k is -9.750989503630585 and b is -58.229024191191165\n",
      "Iteration 436, the loss is 141.64824840385205, parameters k is -9.688143159757066 and b is -58.22900442834531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 437, the loss is 141.25326234717957, parameters k is -9.625296815883548 and b is -58.22898466549946\n",
      "Iteration 438, the loss is 140.85827629050658, parameters k is -9.56245047201003 and b is -58.22896490265361\n",
      "Iteration 439, the loss is 140.4632902338341, parameters k is -9.499604128136511 and b is -58.228945139807756\n",
      "Iteration 440, the loss is 140.0683041771613, parameters k is -9.436757784262992 and b is -58.228925376961904\n",
      "Iteration 441, the loss is 139.67331812048863, parameters k is -9.373911440389474 and b is -58.22890561411605\n",
      "Iteration 442, the loss is 139.27833206381595, parameters k is -9.311065096515955 and b is -58.2288858512702\n",
      "Iteration 443, the loss is 138.88334600714313, parameters k is -9.248218752642437 and b is -58.22886608842435\n",
      "Iteration 444, the loss is 138.48835995047045, parameters k is -9.185372408768918 and b is -58.228846325578495\n",
      "Iteration 445, the loss is 138.09337389379783, parameters k is -9.1225260648954 and b is -58.22882656273264\n",
      "Iteration 446, the loss is 137.69838783712515, parameters k is -9.059679721021881 and b is -58.22880679988679\n",
      "Iteration 447, the loss is 137.30340178045233, parameters k is -8.996833377148363 and b is -58.22878703704094\n",
      "Iteration 448, the loss is 136.90841572377968, parameters k is -8.933987033274844 and b is -58.228767274195086\n",
      "Iteration 449, the loss is 136.51342966710698, parameters k is -8.871140689401326 and b is -58.22874751134923\n",
      "Iteration 450, the loss is 136.11844361043435, parameters k is -8.808294345527807 and b is -58.22872774850338\n",
      "Iteration 451, the loss is 135.72345755376162, parameters k is -8.745448001654289 and b is -58.22870798565753\n",
      "Iteration 452, the loss is 135.32847149708894, parameters k is -8.68260165778077 and b is -58.22868822281168\n",
      "Iteration 453, the loss is 134.93348544041612, parameters k is -8.619755313907252 and b is -58.228668459965824\n",
      "Iteration 454, the loss is 134.53849938374364, parameters k is -8.556908970033733 and b is -58.22864869711997\n",
      "Iteration 455, the loss is 134.1435133270709, parameters k is -8.494062626160215 and b is -58.22862893427412\n",
      "Iteration 456, the loss is 133.74852727039826, parameters k is -8.431216282286696 and b is -58.22860917142827\n",
      "Iteration 457, the loss is 133.35354121372526, parameters k is -8.368369938413178 and b is -58.228589408582415\n",
      "Iteration 458, the loss is 132.95855515705273, parameters k is -8.30552359453966 and b is -58.22856964573656\n",
      "Iteration 459, the loss is 132.56356910037994, parameters k is -8.24267725066614 and b is -58.22854988289071\n",
      "Iteration 460, the loss is 132.16858304370726, parameters k is -8.179830906792622 and b is -58.22853012004486\n",
      "Iteration 461, the loss is 131.7735969870347, parameters k is -8.116984562919104 and b is -58.228510357199006\n",
      "Iteration 462, the loss is 131.37861093036184, parameters k is -8.054138219045585 and b is -58.228490594353154\n",
      "Iteration 463, the loss is 130.98362487368917, parameters k is -7.991291875172068 and b is -58.2284708315073\n",
      "Iteration 464, the loss is 130.5886388170165, parameters k is -7.92844553129855 and b is -58.22845106866145\n",
      "Iteration 465, the loss is 130.1936527603439, parameters k is -7.865599187425032 and b is -58.2284313058156\n",
      "Iteration 466, the loss is 129.79866670367116, parameters k is -7.802752843551515 and b is -58.228411542969745\n",
      "Iteration 467, the loss is 129.4036806469984, parameters k is -7.739906499677997 and b is -58.22839178012389\n",
      "Iteration 468, the loss is 129.00869459032586, parameters k is -7.6770601558044795 and b is -58.22837201727804\n",
      "Iteration 469, the loss is 128.61370853365298, parameters k is -7.614213811930962 and b is -58.22835225443219\n",
      "Iteration 470, the loss is 128.2187224769804, parameters k is -7.551367468057444 and b is -58.228332491586336\n",
      "Iteration 471, the loss is 127.82373642030764, parameters k is -7.488521124183927 and b is -58.228312728740484\n",
      "Iteration 472, the loss is 127.42875036363498, parameters k is -7.425674780310409 and b is -58.22829296589463\n",
      "Iteration 473, the loss is 127.03376430696217, parameters k is -7.362828436436891 and b is -58.22827320304878\n",
      "Iteration 474, the loss is 126.63877825028958, parameters k is -7.299982092563374 and b is -58.22825344020293\n",
      "Iteration 475, the loss is 126.24379219361693, parameters k is -7.237135748689856 and b is -58.228233677357075\n",
      "Iteration 476, the loss is 125.84880613694428, parameters k is -7.174289404816339 and b is -58.22821391451122\n",
      "Iteration 477, the loss is 125.45382008027147, parameters k is -7.111443060942821 and b is -58.22819415166537\n",
      "Iteration 478, the loss is 125.0588340235988, parameters k is -7.048596717069303 and b is -58.22817438881952\n",
      "Iteration 479, the loss is 124.663847966926, parameters k is -6.985750373195786 and b is -58.228154625973666\n",
      "Iteration 480, the loss is 124.26886191025342, parameters k is -6.922904029322268 and b is -58.22813486312781\n",
      "Iteration 481, the loss is 123.87387585358064, parameters k is -6.8600576854487505 and b is -58.22811510028196\n",
      "Iteration 482, the loss is 123.47888979690795, parameters k is -6.797211341575233 and b is -58.22809533743611\n",
      "Iteration 483, the loss is 123.08390374023547, parameters k is -6.734364997701715 and b is -58.22807557459026\n",
      "Iteration 484, the loss is 122.68891768356248, parameters k is -6.671518653828198 and b is -58.228055811744404\n",
      "Iteration 485, the loss is 122.29393162688993, parameters k is -6.60867230995468 and b is -58.22803604889855\n",
      "Iteration 486, the loss is 121.89894557021722, parameters k is -6.545825966081162 and b is -58.2280162860527\n",
      "Iteration 487, the loss is 121.50395951354452, parameters k is -6.482979622207645 and b is -58.22799652320685\n",
      "Iteration 488, the loss is 121.1089734568719, parameters k is -6.420133278334127 and b is -58.227976760360995\n",
      "Iteration 489, the loss is 120.7139874001992, parameters k is -6.3572869344606096 and b is -58.22795699751514\n",
      "Iteration 490, the loss is 120.31900134352648, parameters k is -6.294440590587092 and b is -58.22793723466929\n",
      "Iteration 491, the loss is 119.92401528685383, parameters k is -6.231594246713574 and b is -58.22791747182344\n",
      "Iteration 492, the loss is 119.52902923018095, parameters k is -6.168747902840057 and b is -58.227897708977586\n",
      "Iteration 493, the loss is 119.13404317350832, parameters k is -6.105901558966539 and b is -58.227877946131734\n",
      "Iteration 494, the loss is 118.73905711683584, parameters k is -6.0430552150930215 and b is -58.22785818328588\n",
      "Iteration 495, the loss is 118.34407106016306, parameters k is -5.980208871219504 and b is -58.22783842044003\n",
      "Iteration 496, the loss is 117.94908500349025, parameters k is -5.917362527345986 and b is -58.22781865759418\n",
      "Iteration 497, the loss is 117.55409894681772, parameters k is -5.854516183472469 and b is -58.227798894748325\n",
      "Iteration 498, the loss is 117.15911289014485, parameters k is -5.791669839598951 and b is -58.22777913190247\n",
      "Iteration 499, the loss is 116.76412683347222, parameters k is -5.728823495725433 and b is -58.22775936905662\n",
      "Iteration 500, the loss is 116.36914077679945, parameters k is -5.665977151851916 and b is -58.22773960621077\n",
      "Iteration 501, the loss is 115.97415472012679, parameters k is -5.603130807978398 and b is -58.227719843364916\n",
      "Iteration 502, the loss is 115.57916866345406, parameters k is -5.5402844641048805 and b is -58.227700080519064\n",
      "Iteration 503, the loss is 115.18418260678138, parameters k is -5.477438120231363 and b is -58.22768031767321\n",
      "Iteration 504, the loss is 114.78919655010877, parameters k is -5.414591776357845 and b is -58.22766055482736\n",
      "Iteration 505, the loss is 114.39421049343602, parameters k is -5.351745432484328 and b is -58.22764079198151\n",
      "Iteration 506, the loss is 113.99922443676327, parameters k is -5.28889908861081 and b is -58.227621029135655\n",
      "Iteration 507, the loss is 113.60423838009062, parameters k is -5.226052744737292 and b is -58.2276012662898\n",
      "Iteration 508, the loss is 113.20925232341793, parameters k is -5.163206400863775 and b is -58.22758150344395\n",
      "Iteration 509, the loss is 112.8142662667452, parameters k is -5.100360056990257 and b is -58.2275617405981\n",
      "Iteration 510, the loss is 112.41928021007269, parameters k is -5.03751371311674 and b is -58.227541977752246\n",
      "Iteration 511, the loss is 112.0242941533998, parameters k is -4.974667369243222 and b is -58.227522214906394\n",
      "Iteration 512, the loss is 111.62930809672716, parameters k is -4.911821025369704 and b is -58.22750245206054\n",
      "Iteration 513, the loss is 111.23432204005441, parameters k is -4.848974681496187 and b is -58.22748268921469\n",
      "Iteration 514, the loss is 110.83933598338169, parameters k is -4.786128337622669 and b is -58.22746292636884\n",
      "Iteration 515, the loss is 110.44434992670905, parameters k is -4.7232819937491515 and b is -58.227443163522985\n",
      "Iteration 516, the loss is 110.04936387003626, parameters k is -4.660435649875634 and b is -58.22742340067713\n",
      "Iteration 517, the loss is 109.65437781336372, parameters k is -4.597589306002116 and b is -58.22740363783128\n",
      "Iteration 518, the loss is 109.25939175669093, parameters k is -4.534742962128599 and b is -58.22738387498543\n",
      "Iteration 519, the loss is 108.86440570001835, parameters k is -4.471896618255081 and b is -58.227364112139576\n",
      "Iteration 520, the loss is 108.46941964334562, parameters k is -4.409050274381563 and b is -58.22734434929372\n",
      "Iteration 521, the loss is 108.07443358667274, parameters k is -4.346203930508046 and b is -58.22732458644787\n",
      "Iteration 522, the loss is 107.67944753000016, parameters k is -4.283357586634528 and b is -58.22730482360202\n",
      "Iteration 523, the loss is 107.28446147332744, parameters k is -4.2205112427610105 and b is -58.22728506075617\n",
      "Iteration 524, the loss is 106.88947541665493, parameters k is -4.157664898887493 and b is -58.227265297910314\n",
      "Iteration 525, the loss is 106.49448935998214, parameters k is -4.094818555013975 and b is -58.22724553506446\n",
      "Iteration 526, the loss is 106.09950330330932, parameters k is -4.031972211140458 and b is -58.22722577221861\n",
      "Iteration 527, the loss is 105.70451724663661, parameters k is -3.9691258672669396 and b is -58.22720600937276\n",
      "Iteration 528, the loss is 105.30953118996395, parameters k is -3.9062795233934215 and b is -58.227186246526905\n",
      "Iteration 529, the loss is 104.91454513329137, parameters k is -3.8434331795199035 and b is -58.22716648368105\n",
      "Iteration 530, the loss is 104.51955907661856, parameters k is -3.7805868356463854 and b is -58.2271467208352\n",
      "Iteration 531, the loss is 104.12457301994588, parameters k is -3.7177404917728674 and b is -58.22712695798935\n",
      "Iteration 532, the loss is 103.72958696327319, parameters k is -3.6548941478993493 and b is -58.227107195143496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 533, the loss is 103.33460090660054, parameters k is -3.5920478040258312 and b is -58.227087432297644\n",
      "Iteration 534, the loss is 102.93961484992784, parameters k is -3.529201460152313 and b is -58.22706766945179\n",
      "Iteration 535, the loss is 102.54462879325514, parameters k is -3.466355116278795 and b is -58.22704790660594\n",
      "Iteration 536, the loss is 102.14964273658241, parameters k is -3.403508772405277 and b is -58.22702814376009\n",
      "Iteration 537, the loss is 101.75465667990966, parameters k is -3.340662428531759 and b is -58.227008380914235\n",
      "Iteration 538, the loss is 101.35967062323697, parameters k is -3.277816084658241 and b is -58.22698861806838\n",
      "Iteration 539, the loss is 100.96468456656434, parameters k is -3.214969740784723 and b is -58.22696885522253\n",
      "Iteration 540, the loss is 100.56969850989158, parameters k is -3.152123396911205 and b is -58.22694909237668\n",
      "Iteration 541, the loss is 100.17471245321897, parameters k is -3.0892770530376867 and b is -58.226929329530826\n",
      "Iteration 542, the loss is 99.77972639654622, parameters k is -3.0264307091641687 and b is -58.226909566684974\n",
      "Iteration 543, the loss is 99.38474033987346, parameters k is -2.9635843652906506 and b is -58.22688980383912\n",
      "Iteration 544, the loss is 98.98975428320078, parameters k is -2.9007380214171326 and b is -58.22687004099327\n",
      "Iteration 545, the loss is 98.5947682265281, parameters k is -2.8378916775436145 and b is -58.22685027814742\n",
      "Iteration 546, the loss is 98.19978216985534, parameters k is -2.7750453336700964 and b is -58.226830515301565\n",
      "Iteration 547, the loss is 97.80479611318275, parameters k is -2.7121989897965784 and b is -58.22681075245571\n",
      "Iteration 548, the loss is 97.40981005650995, parameters k is -2.6493526459230603 and b is -58.22679098960986\n",
      "Iteration 549, the loss is 97.01482399983739, parameters k is -2.5865063020495422 and b is -58.22677122676401\n",
      "Iteration 550, the loss is 96.61983794316467, parameters k is -2.523659958176024 and b is -58.226751463918156\n",
      "Iteration 551, the loss is 96.22485188649202, parameters k is -2.460813614302506 and b is -58.2267317010723\n",
      "Iteration 552, the loss is 95.82986582981916, parameters k is -2.397967270428988 and b is -58.22671193822645\n",
      "Iteration 553, the loss is 95.43487977314656, parameters k is -2.33512092655547 and b is -58.2266921753806\n",
      "Iteration 554, the loss is 95.03989371647387, parameters k is -2.272274582681952 and b is -58.22667241253475\n",
      "Iteration 555, the loss is 94.64490765980118, parameters k is -2.209428238808434 and b is -58.226652649688894\n",
      "Iteration 556, the loss is 94.24992160312851, parameters k is -2.146581894934916 and b is -58.22663288684304\n",
      "Iteration 557, the loss is 93.8549355464557, parameters k is -2.0837355510613977 and b is -58.22661312399719\n",
      "Iteration 558, the loss is 93.45994948978303, parameters k is -2.0208892071878797 and b is -58.22659336115134\n",
      "Iteration 559, the loss is 93.06496343311034, parameters k is -1.9580428633143618 and b is -58.226573598305485\n",
      "Iteration 560, the loss is 92.66997737643766, parameters k is -1.895196519440844 and b is -58.22655383545963\n",
      "Iteration 561, the loss is 92.2749913197649, parameters k is -1.8323501755673262 and b is -58.22653407261378\n",
      "Iteration 562, the loss is 91.88000526309222, parameters k is -1.7695038316938083 and b is -58.22651430976793\n",
      "Iteration 563, the loss is 91.48501920641965, parameters k is -1.7066574878202905 and b is -58.226494546922076\n",
      "Iteration 564, the loss is 91.09003314974689, parameters k is -1.6438111439467726 and b is -58.226474784076224\n",
      "Iteration 565, the loss is 90.69504709307417, parameters k is -1.5809648000732548 and b is -58.22645502123037\n",
      "Iteration 566, the loss is 90.30006103640154, parameters k is -1.518118456199737 and b is -58.22643525838452\n",
      "Iteration 567, the loss is 89.90507497972872, parameters k is -1.4552721123262191 and b is -58.22641549553867\n",
      "Iteration 568, the loss is 89.51008892305607, parameters k is -1.3924257684527013 and b is -58.226395732692815\n",
      "Iteration 569, the loss is 89.1151028663833, parameters k is -1.3295794245791834 and b is -58.22637596984696\n",
      "Iteration 570, the loss is 88.72011680971072, parameters k is -1.2667330807056656 and b is -58.22635620700111\n",
      "Iteration 571, the loss is 88.32513075303797, parameters k is -1.2038867368321478 and b is -58.22633644415526\n",
      "Iteration 572, the loss is 87.9301446963653, parameters k is -1.14104039295863 and b is -58.226316681309406\n",
      "Iteration 573, the loss is 87.5351586396926, parameters k is -1.078194049085112 and b is -58.226296918463554\n",
      "Iteration 574, the loss is 87.14017258301989, parameters k is -1.0153477052115942 and b is -58.2262771556177\n",
      "Iteration 575, the loss is 86.74518652634711, parameters k is -0.9525013613380764 and b is -58.22625739277185\n",
      "Iteration 576, the loss is 86.35020046967449, parameters k is -0.8896550174645586 and b is -58.226237629926\n",
      "Iteration 577, the loss is 85.95521441300167, parameters k is -0.8268086735910407 and b is -58.226217867080145\n",
      "Iteration 578, the loss is 85.56022835632915, parameters k is -0.7639623297175229 and b is -58.22619810423429\n",
      "Iteration 579, the loss is 85.16524229965638, parameters k is -0.701115985844005 and b is -58.22617834138844\n",
      "Iteration 580, the loss is 84.77025624298368, parameters k is -0.6382696419704872 and b is -58.22615857854259\n",
      "Iteration 581, the loss is 84.37527018631106, parameters k is -0.5754232980969693 and b is -58.226138815696736\n",
      "Iteration 582, the loss is 83.98028412963838, parameters k is -0.5125769542234515 and b is -58.22611905285088\n",
      "Iteration 583, the loss is 83.58529807296557, parameters k is -0.44973061034993367 and b is -58.22609929000503\n",
      "Iteration 584, the loss is 83.19031201629295, parameters k is -0.38688426647641583 and b is -58.22607952715918\n",
      "Iteration 585, the loss is 82.79532595962014, parameters k is -0.324037922602898 and b is -58.22605976431333\n",
      "Iteration 586, the loss is 82.40033990294748, parameters k is -0.26119157872938015 and b is -58.226040001467474\n",
      "Iteration 587, the loss is 82.00535384627473, parameters k is -0.19834523485586228 and b is -58.22602023862162\n",
      "Iteration 588, the loss is 81.61036778960204, parameters k is -0.1354988909823444 and b is -58.22600047577577\n",
      "Iteration 589, the loss is 81.21538173292943, parameters k is -0.07265254710882654 and b is -58.22598071292992\n",
      "Iteration 590, the loss is 80.82039567625664, parameters k is -0.009806203235308675 and b is -58.225960950084065\n",
      "Iteration 591, the loss is 80.42540961958402, parameters k is 0.05304014063820919 and b is -58.22594118723821\n",
      "Iteration 592, the loss is 80.03042356291134, parameters k is 0.11588648451172706 and b is -58.22592142439236\n",
      "Iteration 593, the loss is 79.63543750623867, parameters k is 0.17873282838524493 and b is -58.22590166154651\n",
      "Iteration 594, the loss is 79.24045144956605, parameters k is 0.2415791722587628 and b is -58.225881898700656\n",
      "Iteration 595, the loss is 78.84546539289329, parameters k is 0.30442551613228064 and b is -58.225862135854804\n",
      "Iteration 596, the loss is 78.45047933622052, parameters k is 0.3672718600057985 and b is -58.22584237300895\n",
      "Iteration 597, the loss is 78.05549327954779, parameters k is 0.4301182038793163 and b is -58.2258226101631\n",
      "Iteration 598, the loss is 77.6605072228752, parameters k is 0.49296454775283416 and b is -58.22580284731725\n",
      "Iteration 599, the loss is 77.26552116620243, parameters k is 0.555810891626352 and b is -58.225783084471395\n",
      "Iteration 600, the loss is 76.8705351095297, parameters k is 0.6186572354998698 and b is -58.22576332162554\n",
      "Iteration 601, the loss is 76.475549052857, parameters k is 0.6815035793733877 and b is -58.22574355877969\n",
      "Iteration 602, the loss is 76.08056299618438, parameters k is 0.7443499232469055 and b is -58.22572379593384\n",
      "Iteration 603, the loss is 75.68557693951173, parameters k is 0.8071962671204234 and b is -58.225704033087986\n",
      "Iteration 604, the loss is 75.29059088283894, parameters k is 0.8700426109939412 and b is -58.225684270242134\n",
      "Iteration 605, the loss is 74.89560482616629, parameters k is 0.932888954867459 and b is -58.22566450739628\n",
      "Iteration 606, the loss is 74.50061876949358, parameters k is 0.9957352987409769 and b is -58.22564474455043\n",
      "Iteration 607, the loss is 74.10563271282082, parameters k is 1.0585816426144947 and b is -58.22562498170458\n",
      "Iteration 608, the loss is 73.71064665614826, parameters k is 1.1214279864880126 and b is -58.225605218858725\n",
      "Iteration 609, the loss is 73.31566059947548, parameters k is 1.1842743303615304 and b is -58.22558545601287\n",
      "Iteration 610, the loss is 72.92067454280274, parameters k is 1.2471206742350482 and b is -58.22556569316702\n",
      "Iteration 611, the loss is 72.52568848613004, parameters k is 1.309967018108566 and b is -58.22554593032117\n",
      "Iteration 612, the loss is 72.13070242945732, parameters k is 1.372813361982084 and b is -58.225526167475316\n",
      "Iteration 613, the loss is 71.73571637278472, parameters k is 1.4356597058556018 and b is -58.22550640462946\n",
      "Iteration 614, the loss is 71.340730316112, parameters k is 1.4985060497291196 and b is -58.22548664178361\n",
      "Iteration 615, the loss is 70.94574425943927, parameters k is 1.5613523936026374 and b is -58.22546687893776\n",
      "Iteration 616, the loss is 70.55075820276657, parameters k is 1.6241987374761553 and b is -58.22544711609191\n",
      "Iteration 617, the loss is 70.15577214609384, parameters k is 1.6870450813496731 and b is -58.225427353246054\n",
      "Iteration 618, the loss is 69.76078608942126, parameters k is 1.749891425223191 and b is -58.2254075904002\n",
      "Iteration 619, the loss is 69.3658000327485, parameters k is 1.8127377690967088 and b is -58.22538782755435\n",
      "Iteration 620, the loss is 68.97081397607573, parameters k is 1.8755841129702266 and b is -58.2253680647085\n",
      "Iteration 621, the loss is 68.57582791940311, parameters k is 1.9384304568437445 and b is -58.225348301862645\n",
      "Iteration 622, the loss is 68.1808418627304, parameters k is 2.0012768007172625 and b is -58.22532853901679\n",
      "Iteration 623, the loss is 67.78585580605773, parameters k is 2.0641231445907806 and b is -58.22530877617094\n",
      "Iteration 624, the loss is 67.39086974938502, parameters k is 2.1269694884642987 and b is -58.22528901332509\n",
      "Iteration 625, the loss is 66.99588369271234, parameters k is 2.1898158323378167 and b is -58.225269250479236\n",
      "Iteration 626, the loss is 66.60089763603958, parameters k is 2.252662176211335 and b is -58.225249487633384\n",
      "Iteration 627, the loss is 66.20591157936688, parameters k is 2.315508520084853 and b is -58.22522972478753\n",
      "Iteration 628, the loss is 65.81092552269426, parameters k is 2.378354863958371 and b is -58.22520996194168\n",
      "Iteration 629, the loss is 65.4159394660215, parameters k is 2.441201207831889 and b is -58.22519019909583\n",
      "Iteration 630, the loss is 65.02095340934885, parameters k is 2.504047551705407 and b is -58.225170436249975\n",
      "Iteration 631, the loss is 64.62596735267613, parameters k is 2.566893895578925 and b is -58.22515067340412\n",
      "Iteration 632, the loss is 64.23098129600342, parameters k is 2.629740239452443 and b is -58.22513091055827\n",
      "Iteration 633, the loss is 63.835995239330714, parameters k is 2.6925865833259612 and b is -58.22511114771242\n",
      "Iteration 634, the loss is 63.44100918265799, parameters k is 2.7554329271994793 and b is -58.225091384866566\n",
      "Iteration 635, the loss is 63.04602312598534, parameters k is 2.8182792710729974 and b is -58.225071622020714\n",
      "Iteration 636, the loss is 62.651037069312636, parameters k is 2.8811256149465154 and b is -58.22505185917486\n",
      "Iteration 637, the loss is 62.25605101263988, parameters k is 2.9439719588200335 and b is -58.22503209632901\n",
      "Iteration 638, the loss is 61.86106495596725, parameters k is 3.0068183026935515 and b is -58.22501233348316\n",
      "Iteration 639, the loss is 61.46607889929455, parameters k is 3.0696646465670696 and b is -58.224992570637305\n",
      "Iteration 640, the loss is 61.07109284262184, parameters k is 3.1325109904405877 and b is -58.22497280779145\n",
      "Iteration 641, the loss is 60.676106785949116, parameters k is 3.1953573343141057 and b is -58.2249530449456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 642, the loss is 60.28112072927644, parameters k is 3.258203678187624 and b is -58.22493328209975\n",
      "Iteration 643, the loss is 59.8861346726037, parameters k is 3.321050022061142 and b is -58.224913519253896\n",
      "Iteration 644, the loss is 59.49114861593102, parameters k is 3.38389636593466 and b is -58.22489375640804\n",
      "Iteration 645, the loss is 59.09616255925831, parameters k is 3.446742709808178 and b is -58.22487399356219\n",
      "Iteration 646, the loss is 58.70117650258565, parameters k is 3.509589053681696 and b is -58.22485423071634\n",
      "Iteration 647, the loss is 58.3061904459129, parameters k is 3.572435397555214 and b is -58.22483446787049\n",
      "Iteration 648, the loss is 57.91120438924018, parameters k is 3.635281741428732 and b is -58.224814705024635\n",
      "Iteration 649, the loss is 57.516218332567554, parameters k is 3.6981280853022502 and b is -58.22479494217878\n",
      "Iteration 650, the loss is 57.12123227589479, parameters k is 3.7609744291757683 and b is -58.22477517933293\n",
      "Iteration 651, the loss is 56.72624621922214, parameters k is 3.8238207730492864 and b is -58.22475541648708\n",
      "Iteration 652, the loss is 56.33126016254944, parameters k is 3.8866671169228044 and b is -58.224735653641225\n",
      "Iteration 653, the loss is 55.936274105876755, parameters k is 3.9495134607963225 and b is -58.22471589079537\n",
      "Iteration 654, the loss is 55.541288049204034, parameters k is 4.01235980466984 and b is -58.22469612794952\n",
      "Iteration 655, the loss is 55.146301992531356, parameters k is 4.075206148543358 and b is -58.22467636510367\n",
      "Iteration 656, the loss is 54.7513159358586, parameters k is 4.138052492416875 and b is -58.22465660225782\n",
      "Iteration 657, the loss is 54.35632987918592, parameters k is 4.200898836290393 and b is -58.224636839411964\n",
      "Iteration 658, the loss is 53.96134382251324, parameters k is 4.263745180163911 and b is -58.22461707656611\n",
      "Iteration 659, the loss is 53.56635776584056, parameters k is 4.326591524037428 and b is -58.22459731372026\n",
      "Iteration 660, the loss is 53.17137170916786, parameters k is 4.389437867910946 and b is -58.22457755087441\n",
      "Iteration 661, the loss is 52.77638565249515, parameters k is 4.452284211784463 and b is -58.224557788028555\n",
      "Iteration 662, the loss is 52.3813995958225, parameters k is 4.515130555657981 and b is -58.2245380251827\n",
      "Iteration 663, the loss is 51.98641353914978, parameters k is 4.577976899531499 and b is -58.22451826233685\n",
      "Iteration 664, the loss is 51.59142748247714, parameters k is 4.640823243405016 and b is -58.224498499491\n",
      "Iteration 665, the loss is 51.19644142580437, parameters k is 4.703669587278534 and b is -58.224478736645146\n",
      "Iteration 666, the loss is 50.801455369131666, parameters k is 4.7665159311520515 and b is -58.224458973799294\n",
      "Iteration 667, the loss is 50.406469312459016, parameters k is 4.829362275025569 and b is -58.22443921095344\n",
      "Iteration 668, the loss is 50.011483255786345, parameters k is 4.892208618899087 and b is -58.22441944810759\n",
      "Iteration 669, the loss is 49.616497199113624, parameters k is 4.955054962772604 and b is -58.22439968526174\n",
      "Iteration 670, the loss is 49.2215111424409, parameters k is 5.017901306646122 and b is -58.224379922415885\n",
      "Iteration 671, the loss is 48.826525085768196, parameters k is 5.08074765051964 and b is -58.22436015957003\n",
      "Iteration 672, the loss is 48.43153902909556, parameters k is 5.143593994393157 and b is -58.22434039672418\n",
      "Iteration 673, the loss is 48.03655297242282, parameters k is 5.206440338266675 and b is -58.22432063387833\n",
      "Iteration 674, the loss is 47.641566915750126, parameters k is 5.2692866821401925 and b is -58.224300871032476\n",
      "Iteration 675, the loss is 47.24658085907741, parameters k is 5.33213302601371 and b is -58.224281108186624\n",
      "Iteration 676, the loss is 46.851594802404726, parameters k is 5.394979369887228 and b is -58.22426134534077\n",
      "Iteration 677, the loss is 46.4566087457321, parameters k is 5.457825713760745 and b is -58.22424158249492\n",
      "Iteration 678, the loss is 46.0616226890593, parameters k is 5.520672057634263 and b is -58.22422181964907\n",
      "Iteration 679, the loss is 45.666636632386655, parameters k is 5.583518401507781 and b is -58.224202056803215\n",
      "Iteration 680, the loss is 45.271650575713934, parameters k is 5.646364745381298 and b is -58.22418229395736\n",
      "Iteration 681, the loss is 44.87666451904133, parameters k is 5.709211089254816 and b is -58.22416253111151\n",
      "Iteration 682, the loss is 44.48167846236856, parameters k is 5.772057433128333 and b is -58.22414276826566\n",
      "Iteration 683, the loss is 44.08669240569589, parameters k is 5.834903777001851 and b is -58.224123005419806\n",
      "Iteration 684, the loss is 43.6917063490232, parameters k is 5.897750120875369 and b is -58.22410324257395\n",
      "Iteration 685, the loss is 43.29672029235049, parameters k is 5.960596464748886 and b is -58.2240834797281\n",
      "Iteration 686, the loss is 42.90173423567776, parameters k is 6.023442808622404 and b is -58.22406371688225\n",
      "Iteration 687, the loss is 42.506748179005065, parameters k is 6.0862891524959215 and b is -58.2240439540364\n",
      "Iteration 688, the loss is 42.11176212233239, parameters k is 6.149135496369439 and b is -58.224024191190544\n",
      "Iteration 689, the loss is 41.716776065659715, parameters k is 6.211981840242957 and b is -58.22400442834469\n",
      "Iteration 690, the loss is 41.32179000898698, parameters k is 6.274828184116474 and b is -58.22398466549884\n",
      "Iteration 691, the loss is 40.92680395231433, parameters k is 6.337674527989992 and b is -58.22396490265299\n",
      "Iteration 692, the loss is 40.53181789564162, parameters k is 6.40052087186351 and b is -58.223945139807135\n",
      "Iteration 693, the loss is 40.1368318389689, parameters k is 6.463367215737027 and b is -58.22392537696128\n",
      "Iteration 694, the loss is 39.741845782296224, parameters k is 6.526213559610545 and b is -58.22390561411543\n",
      "Iteration 695, the loss is 39.34685972562352, parameters k is 6.5890599034840625 and b is -58.22388585126958\n",
      "Iteration 696, the loss is 38.9518736689508, parameters k is 6.65190624735758 and b is -58.223866088423726\n",
      "Iteration 697, the loss is 38.556887612278146, parameters k is 6.714752591231098 and b is -58.223846325577874\n",
      "Iteration 698, the loss is 38.16190155560545, parameters k is 6.777598935104615 and b is -58.22382656273202\n",
      "Iteration 699, the loss is 37.76691549893273, parameters k is 6.840445278978133 and b is -58.22380679988617\n",
      "Iteration 700, the loss is 37.371929442260054, parameters k is 6.9032916228516505 and b is -58.22378703704032\n",
      "Iteration 701, the loss is 36.97694338558736, parameters k is 6.966137966725168 and b is -58.223767274194465\n",
      "Iteration 702, the loss is 36.58195732891464, parameters k is 7.028984310598686 and b is -58.22374751134861\n",
      "Iteration 703, the loss is 36.186971272241934, parameters k is 7.091830654472203 and b is -58.22372774850276\n",
      "Iteration 704, the loss is 35.791985215569255, parameters k is 7.154676998345721 and b is -58.22370798565691\n",
      "Iteration 705, the loss is 35.396999158896556, parameters k is 7.217523342219239 and b is -58.223688222811056\n",
      "Iteration 706, the loss is 35.00201310222384, parameters k is 7.280369686092756 and b is -58.223668459965204\n",
      "Iteration 707, the loss is 34.60702704555116, parameters k is 7.343216029966274 and b is -58.22364869711935\n",
      "Iteration 708, the loss is 34.21204098887848, parameters k is 7.4060623738397915 and b is -58.2236289342735\n",
      "Iteration 709, the loss is 33.81705493220578, parameters k is 7.468908717713309 and b is -58.22360917142765\n",
      "Iteration 710, the loss is 33.422068875533085, parameters k is 7.531755061586827 and b is -58.223589408581795\n",
      "Iteration 711, the loss is 33.02708281886037, parameters k is 7.594601405460344 and b is -58.22356964573594\n",
      "Iteration 712, the loss is 32.63209676218771, parameters k is 7.657447749333862 and b is -58.22354988289009\n",
      "Iteration 713, the loss is 32.237110705514986, parameters k is 7.72029409320738 and b is -58.22353012004424\n",
      "Iteration 714, the loss is 31.84212464884231, parameters k is 7.783140437080897 and b is -58.223510357198386\n",
      "Iteration 715, the loss is 31.447138592169612, parameters k is 7.845986780954415 and b is -58.22349059435253\n",
      "Iteration 716, the loss is 31.05215253549692, parameters k is 7.908833124827932 and b is -58.22347083150668\n",
      "Iteration 717, the loss is 30.657166478824234, parameters k is 7.97167946870145 and b is -58.22345106866083\n",
      "Iteration 718, the loss is 30.262180422151552, parameters k is 8.034525812574968 and b is -58.22343130581498\n",
      "Iteration 719, the loss is 29.86719436547881, parameters k is 8.097372156448486 and b is -58.223411542969124\n",
      "Iteration 720, the loss is 29.472208308806124, parameters k is 8.160218500322005 and b is -58.22339178012327\n",
      "Iteration 721, the loss is 29.077222252133403, parameters k is 8.223064844195523 and b is -58.22337201727742\n",
      "Iteration 722, the loss is 28.68223619546072, parameters k is 8.285911188069042 and b is -58.22335225443157\n",
      "Iteration 723, the loss is 28.287250138788025, parameters k is 8.34875753194256 and b is -58.223332491585715\n",
      "Iteration 724, the loss is 27.89226408211531, parameters k is 8.411603875816079 and b is -58.22331272873986\n",
      "Iteration 725, the loss is 27.497278025442625, parameters k is 8.474450219689597 and b is -58.22329296589401\n",
      "Iteration 726, the loss is 27.1022919687699, parameters k is 8.537296563563116 and b is -58.22327320304816\n",
      "Iteration 727, the loss is 26.707305912097198, parameters k is 8.600142907436634 and b is -58.223253440202306\n",
      "Iteration 728, the loss is 26.312319855424533, parameters k is 8.662989251310153 and b is -58.223233677356454\n",
      "Iteration 729, the loss is 25.91733379875183, parameters k is 8.725835595183671 and b is -58.2232139145106\n",
      "Iteration 730, the loss is 25.52234774207911, parameters k is 8.78868193905719 and b is -58.22319415166475\n",
      "Iteration 731, the loss is 25.12736168540638, parameters k is 8.851528282930708 and b is -58.2231743888189\n",
      "Iteration 732, the loss is 24.73237562873366, parameters k is 8.914374626804227 and b is -58.223154625973045\n",
      "Iteration 733, the loss is 24.33738957206097, parameters k is 8.977220970677745 and b is -58.22313486312719\n",
      "Iteration 734, the loss is 23.942403515388293, parameters k is 9.040067314551264 and b is -58.22311510028134\n",
      "Iteration 735, the loss is 23.54741745871559, parameters k is 9.102913658424782 and b is -58.22309533743549\n",
      "Iteration 736, the loss is 23.153823881274477, parameters k is 9.1657600022983 and b is -58.223075574589636\n",
      "Iteration 737, the loss is 22.76318784273989, parameters k is 9.228259310598697 and b is -58.223055811743784\n",
      "Iteration 738, the loss is 22.372551804205244, parameters k is 9.290758618899092 and b is -58.22303604889793\n",
      "Iteration 739, the loss is 21.981915765670582, parameters k is 9.353257927199488 and b is -58.22301628605208\n",
      "Iteration 740, the loss is 21.591279727135966, parameters k is 9.415757235499884 and b is -58.22299652320623\n",
      "Iteration 741, the loss is 21.200643688601332, parameters k is 9.47825654380028 and b is -58.222976760360375\n",
      "Iteration 742, the loss is 20.81000765006673, parameters k is 9.540755852100675 and b is -58.22295699751452\n",
      "Iteration 743, the loss is 20.419371611532096, parameters k is 9.603255160401071 and b is -58.22293723466867\n",
      "Iteration 744, the loss is 20.028735572997444, parameters k is 9.665754468701467 and b is -58.22291747182282\n",
      "Iteration 745, the loss is 19.639103958995047, parameters k is 9.728253777001862 and b is -58.222897708976966\n",
      "Iteration 746, the loss is 19.252552245441226, parameters k is 9.790485022061151 and b is -58.22287794613111\n",
      "Iteration 747, the loss is 18.868724281662036, parameters k is 9.852446543800282 and b is -58.22285818328526\n",
      "Iteration 748, the loss is 18.48818725505293, parameters k is 9.91413265051965 and b is -58.22283842043941\n",
      "Iteration 749, the loss is 18.107650228443852, parameters k is 9.975818757239017 and b is -58.22281865759356\n",
      "Iteration 750, the loss is 17.72783049106547, parameters k is 10.037504863958384 and b is -58.222798894747704\n",
      "Iteration 751, the loss is 17.35085127839688, parameters k is 10.098901919294352 and b is -58.22277913190185\n",
      "Iteration 752, the loss is 16.97387206572832, parameters k is 10.16029897463032 and b is -58.222759369056\n",
      "Iteration 753, the loss is 16.598251163931646, parameters k is 10.221696029966289 and b is -58.22273960621015\n",
      "Iteration 754, the loss is 16.224388318264293, parameters k is 10.282838777001862 and b is -58.222719843364295\n",
      "Iteration 755, the loss is 15.853053259368233, parameters k is 10.343981524037435 and b is -58.22270008051844\n",
      "Iteration 756, the loss is 15.485800277162568, parameters k is 10.404581346171822 and b is -58.22268031767259\n",
      "Iteration 757, the loss is 15.118547294956901, parameters k is 10.46518116830621 and b is -58.22266055482674\n",
      "Iteration 758, the loss is 14.752023285823377, parameters k is 10.525780990440596 and b is -58.222640791980886\n",
      "Iteration 759, the loss is 14.390133544379736, parameters k is 10.586125437080913 and b is -58.222621029135034\n",
      "Iteration 760, the loss is 14.037195920287344, parameters k is 10.645672571468264 and b is -58.22260126628918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 761, the loss is 13.689797602432847, parameters k is 10.704700773049291 and b is -58.22258150344333\n",
      "Iteration 762, the loss is 13.344134243701681, parameters k is 10.763492334314114 and b is -58.22256174059748\n",
      "Iteration 763, the loss is 12.998938782069501, parameters k is 10.822283895578936 and b is -58.222541977751625\n",
      "Iteration 764, the loss is 12.658643930157485, parameters k is 10.880813441033482 and b is -58.22252221490577\n",
      "Iteration 765, the loss is 12.327973683584322, parameters k is 10.938315792812137 and b is -58.22250245205992\n",
      "Iteration 766, the loss is 11.99769565176734, parameters k is 10.995818144590793 and b is -58.22248268921407\n",
      "Iteration 767, the loss is 11.67496934166028, parameters k is 11.052821405460358 and b is -58.222462926368216\n",
      "Iteration 768, the loss is 11.362243056603505, parameters k is 11.108821385697512 and b is -58.222443163522364\n",
      "Iteration 769, the loss is 11.056433905112947, parameters k is 11.16430982443269 and b is -58.22242340067651\n",
      "Iteration 770, the loss is 10.758644158604582, parameters k is 11.21906363866194 and b is -58.22240363783066\n",
      "Iteration 771, the loss is 10.466272621778808, parameters k is 11.273320496369449 and b is -58.22238387498481\n",
      "Iteration 772, the loss is 10.183329679085691, parameters k is 11.326607551705417 and b is -58.222364112138955\n",
      "Iteration 773, the loss is 9.905076208078723, parameters k is 11.379661563563124 and b is -58.2223443492931\n",
      "Iteration 774, the loss is 9.631829952908488, parameters k is 11.4319490536817 and b is -58.22232458644725\n",
      "Iteration 775, the loss is 9.361858948003865, parameters k is 11.483978045776562 and b is -58.2223048236014\n",
      "Iteration 776, the loss is 9.096445699762576, parameters k is 11.535504073444546 and b is -58.222285060755546\n",
      "Iteration 777, the loss is 8.83481404394145, parameters k is 11.586779468701463 and b is -58.22226529790969\n",
      "Iteration 778, the loss is 8.580776967118295, parameters k is 11.63754462680423 and b is -58.22224553506384\n",
      "Iteration 779, the loss is 8.34060558288606, parameters k is 11.686618757239012 and b is -58.22222577221799\n",
      "Iteration 780, the loss is 8.109631036494532, parameters k is 11.73490743312834 and b is -58.22220600937214\n",
      "Iteration 781, the loss is 7.886791523802968, parameters k is 11.782476899531503 and b is -58.222186246526284\n",
      "Iteration 782, the loss is 7.678964914019369, parameters k is 11.828548500322016 and b is -58.22216648368043\n",
      "Iteration 783, the loss is 7.4811464135071635, parameters k is 11.873105516132293 and b is -58.22214672083458\n",
      "Iteration 784, the loss is 7.289501224745796, parameters k is 11.91690834221925 and b is -58.22212695798873\n",
      "Iteration 785, the loss is 7.101318183125581, parameters k is 11.960459152495929 and b is -58.222107195142875\n",
      "Iteration 786, the loss is 6.918436341224549, parameters k is 12.003449409412925 and b is -58.22208743229702\n",
      "Iteration 787, the loss is 6.747906067976339, parameters k is 12.045118559610554 and b is -58.22206766945117\n",
      "Iteration 788, the loss is 6.588498796171151, parameters k is 12.085739686092767 and b is -58.22204790660532\n",
      "Iteration 789, the loss is 6.442550984066243, parameters k is 12.123941207831898 and b is -58.222028143759466\n",
      "Iteration 790, the loss is 6.298631946374674, parameters k is 12.162142729571029 and b is -58.222008380913614\n",
      "Iteration 791, the loss is 6.160703110819203, parameters k is 12.199480496369448 and b is -58.22198861806776\n",
      "Iteration 792, the loss is 6.0298864740997855, parameters k is 12.235977354076958 and b is -58.22196885522191\n",
      "Iteration 793, the loss is 5.90936364914667, parameters k is 12.270844409412927 and b is -58.22194909237606\n",
      "Iteration 794, the loss is 5.794616983575023, parameters k is 12.304954073444547 and b is -58.221929329530205\n",
      "Iteration 795, the loss is 5.6866800634603445, parameters k is 12.33804195882004 and b is -58.22190956668435\n",
      "Iteration 796, the loss is 5.584159901535554, parameters k is 12.370323638661938 and b is -58.2218898038385\n",
      "Iteration 797, the loss is 5.485505513124042, parameters k is 12.401807808622413 and b is -58.22187004099265\n",
      "Iteration 798, the loss is 5.392164091717315, parameters k is 12.432805101112532 and b is -58.221850278146796\n",
      "Iteration 799, the loss is 5.308495987946526, parameters k is 12.462222907436642 and b is -58.221830515300944\n",
      "Iteration 800, the loss is 5.238017983339944, parameters k is 12.489074705855614 and b is -58.22181075245509\n",
      "Iteration 801, the loss is 5.174256554922627, parameters k is 12.515079982535456 and b is -58.22179098960924\n",
      "Iteration 802, the loss is 5.122470552281496, parameters k is 12.538059844195535 and b is -58.22181075245509\n",
      "Iteration 803, the loss is 5.0749787036788, parameters k is 12.560020693997906 and b is -58.221830515300944\n",
      "Iteration 804, the loss is 5.032837422969203, parameters k is 12.580905377792373 and b is -58.221850278146796\n",
      "Iteration 805, the loss is 4.996387524275771, parameters k is 12.600255950914903 and b is -58.22187004099265\n",
      "Iteration 806, the loss is 4.963061643274018, parameters k is 12.618611069491978 and b is -58.2218898038385\n",
      "Iteration 807, the loss is 4.933617004239427, parameters k is 12.636172591231109 and b is -58.22190956668435\n",
      "Iteration 808, the loss is 4.909130804244315, parameters k is 12.65244644498605 and b is -58.221929329530205\n",
      "Iteration 809, the loss is 4.889540318043579, parameters k is 12.666444804669844 and b is -58.22194909237606\n",
      "Iteration 810, the loss is 4.870678213324306, parameters k is 12.680443164353639 and b is -58.22196885522191\n",
      "Iteration 811, the loss is 4.853182486134023, parameters k is 12.693672077397117 and b is -58.22198861806776\n",
      "Iteration 812, the loss is 4.836045025076103, parameters k is 12.706900990440596 and b is -58.222008380913614\n",
      "Iteration 813, the loss is 4.8198325017545205, parameters k is 12.71964330269356 and b is -58.222028143759466\n",
      "Iteration 814, the loss is 4.804554196641338, parameters k is 12.73215806553941 and b is -58.22204790660532\n",
      "Iteration 815, the loss is 4.79078313878266, parameters k is 12.744187176211348 and b is -58.22206766945117\n",
      "Iteration 816, the loss is 4.780764638118001, parameters k is 12.754612492416882 and b is -58.22208743229702\n",
      "Iteration 817, the loss is 4.773107700238526, parameters k is 12.763532512179728 and b is -58.222107195142875\n",
      "Iteration 818, the loss is 4.766751594685562, parameters k is 12.77163454775285 and b is -58.22212695798873\n",
      "Iteration 819, the loss is 4.761209830528781, parameters k is 12.779200575420834 and b is -58.22214672083458\n",
      "Iteration 820, the loss is 4.756432013884292, parameters k is 12.786263717713323 and b is -58.22216648368043\n",
      "Iteration 821, the loss is 4.752249979122081, parameters k is 12.792823302693561 and b is -58.222186246526284\n",
      "Iteration 822, the loss is 4.748393056951478, parameters k is 12.799149765144154 and b is -58.22220600937214\n",
      "Iteration 823, the loss is 4.744717853234773, parameters k is 12.80521417225878 and b is -58.22222577221799\n",
      "Iteration 824, the loss is 4.74104264951807, parameters k is 12.811278579373404 and b is -58.22224553506384\n",
      "Iteration 825, the loss is 4.737442581717982, parameters k is 12.817342986488029 and b is -58.22226529790969\n",
      "Iteration 826, the loss is 4.7343693020557085, parameters k is 12.822888816527556 and b is -58.222285060755546\n",
      "Iteration 827, the loss is 4.731534913040145, parameters k is 12.828434646567082 and b is -58.2223048236014\n",
      "Iteration 828, the loss is 4.728997600678899, parameters k is 12.83350751217973 and b is -58.22232458644725\n",
      "Iteration 829, the loss is 4.726849201833129, parameters k is 12.838338203879335 and b is -58.2223443492931\n",
      "Iteration 830, the loss is 4.724992047018921, parameters k is 12.842705575420837 and b is -58.222364112138955\n",
      "Iteration 831, the loss is 4.723449788505433, parameters k is 12.846792156448505 and b is -58.22238387498481\n",
      "Iteration 832, the loss is 4.722375859070831, parameters k is 12.850116642614513 and b is -58.22240363783066\n",
      "Iteration 833, the loss is 4.72152581790124, parameters k is 12.853151286883287 and b is -58.22242340067651\n",
      "Iteration 834, the loss is 4.720832993318114, parameters k is 12.855945022061153 and b is -58.222443163522364\n",
      "Iteration 835, the loss is 4.7203018884325125, parameters k is 12.858252472654039 and b is -58.222462926368216\n",
      "Iteration 836, the loss is 4.719868741264097, parameters k is 12.860559923246925 and b is -58.22248268921407\n",
      "Iteration 837, the loss is 4.7196230034378175, parameters k is 12.862131010203447 and b is -58.22250245205992\n",
      "Iteration 838, the loss is 4.719383788291073, parameters k is 12.863702097159969 and b is -58.22252221490577\n",
      "Iteration 839, the loss is 4.7192077192138715, parameters k is 12.865032828385266 and b is -58.222541977751625\n",
      "Iteration 840, the loss is 4.719051925413067, parameters k is 12.866363559610562 and b is -58.22256174059748\n",
      "Iteration 841, the loss is 4.71893011302258, parameters k is 12.867471484511748 and b is -58.22258150344333\n",
      "Iteration 842, the loss is 4.718823604845236, parameters k is 12.868579409412934 and b is -58.22260126628918\n",
      "Iteration 843, the loss is 4.718750772393201, parameters k is 12.869437848148111 and b is -58.222621029135034\n",
      "Iteration 844, the loss is 4.718681807172318, parameters k is 12.870296286883288 and b is -58.222640791980886\n",
      "Iteration 845, the loss is 4.718651827319616, parameters k is 12.870912037871431 and b is -58.22266055482674\n",
      "Iteration 846, the loss is 4.7186382894177195, parameters k is 12.871289409412933 and b is -58.22268031767259\n",
      "Iteration 847, the loss is 4.718625933006612, parameters k is 12.871666780954435 and b is -58.22270008051844\n",
      "Iteration 848, the loss is 4.718624379086035, parameters k is 12.871814389650087 and b is -58.222719843364295\n",
      "Iteration 849, the loss is 4.718622971940434, parameters k is 12.87196199834574 and b is -58.22273960621015\n",
      "Iteration 850, the loss is 4.718624698917148, parameters k is 12.871877393602656 and b is -58.222759369056\n",
      "Iteration 851, the loss is 4.71862459857997, parameters k is 12.872025002298308 and b is -58.22277913190185\n",
      "Iteration 852, the loss is 4.71862501874827, parameters k is 12.871940397555225 and b is -58.222798894747704\n",
      "Iteration 853, the loss is 4.718626225219506, parameters k is 12.872088006250877 and b is -58.22281865759356\n",
      "Iteration 854, the loss is 4.718626056221354, parameters k is 12.872003401507794 and b is -58.22283842043941\n",
      "Iteration 855, the loss is 4.718627212331076, parameters k is 12.87191879676471 and b is -58.22285818328526\n",
      "Iteration 856, the loss is 4.718627682860891, parameters k is 12.872066405460362 and b is -58.22287794613111\n",
      "Iteration 857, the loss is 4.718627532162201, parameters k is 12.871981800717279 and b is -58.222897708976966\n",
      "Iteration 858, the loss is 4.718629309500426, parameters k is 12.872129409412931 and b is -58.22291747182282\n",
      "Iteration 859, the loss is 4.7186291405022756, parameters k is 12.872044804669848 and b is -58.22293723466867\n",
      "Iteration 860, the loss is 4.71862972574502, parameters k is 12.871960199926765 and b is -58.22295699751452\n",
      "Iteration 861, the loss is 4.718630767141811, parameters k is 12.872107808622417 and b is -58.222976760360375\n",
      "Iteration 862, the loss is 4.718630598143664, parameters k is 12.872023203879333 and b is -58.22299652320623\n",
      "Iteration 863, the loss is 4.718631919327838, parameters k is 12.87193859913625 and b is -58.22301628605208\n",
      "Iteration 864, the loss is 4.718632224783202, parameters k is 12.872086207831902 and b is -58.22303604889793\n",
      "Iteration 865, the loss is 4.718632239158947, parameters k is 12.872001603088819 and b is -58.223055811743784\n",
      "Iteration 866, the loss is 4.718633851422739, parameters k is 12.872149211784471 and b is -58.223075574589636\n",
      "Iteration 867, the loss is 4.718633682424587, parameters k is 12.872064607041388 and b is -58.22309533743549\n",
      "Iteration 868, the loss is 4.718634432741768, parameters k is 12.871980002298304 and b is -58.22311510028134\n",
      "Iteration 869, the loss is 4.718635309064125, parameters k is 12.872127610993957 and b is -58.22313486312719\n",
      "Iteration 870, the loss is 4.718635140065975, parameters k is 12.872043006250873 and b is -58.223154625973045\n",
      "Iteration 871, the loss is 4.718636626324587, parameters k is 12.87195840150779 and b is -58.2231743888189\n",
      "Iteration 872, the loss is 4.718636766705507, parameters k is 12.872106010203442 and b is -58.22319415166475\n",
      "Iteration 873, the loss is 4.718636946155706, parameters k is 12.872021405460359 and b is -58.2232139145106\n",
      "Iteration 874, the loss is 4.718638393345046, parameters k is 12.872169014156011 and b is -58.223233677356454\n",
      "Iteration 875, the loss is 4.718638224346897, parameters k is 12.872084409412928 and b is -58.223253440202306\n",
      "Iteration 876, the loss is 4.718639139738517, parameters k is 12.871999804669844 and b is -58.22327320304816\n",
      "Iteration 877, the loss is 4.718639850986435, parameters k is 12.872147413365497 and b is -58.22329296589401\n",
      "Iteration 878, the loss is 4.718639681988281, parameters k is 12.872062808622413 and b is -58.22331272873986\n",
      "Iteration 879, the loss is 4.71864133332134, parameters k is 12.87197820387933 and b is -58.223332491585715\n",
      "Iteration 880, the loss is 4.718641308627819, parameters k is 12.872125812574982 and b is -58.22335225443157\n",
      "Iteration 881, the loss is 4.71864165315246, parameters k is 12.872041207831899 and b is -58.22337201727742\n",
      "Iteration 882, the loss is 4.718642935267351, parameters k is 12.87218881652755 and b is -58.22339178012327\n",
      "Iteration 883, the loss is 4.718642766269203, parameters k is 12.872104211784468 and b is -58.223411542969124\n",
      "Iteration 884, the loss is 4.7186438467352705, parameters k is 12.872019607041384 and b is -58.22343130581498\n",
      "Iteration 885, the loss is 4.71864439290874, parameters k is 12.872167215737036 and b is -58.22345106866083\n",
      "Iteration 886, the loss is 4.718644223910588, parameters k is 12.872082610993953 and b is -58.22347083150668\n",
      "Iteration 887, the loss is 4.718646040318086, parameters k is 12.87199800625087 and b is -58.22349059435253\n",
      "Iteration 888, the loss is 4.718645850550125, parameters k is 12.872145614946522 and b is -58.223510357198386\n",
      "Iteration 889, the loss is 4.718646360149206, parameters k is 12.872061010203439 and b is -58.22353012004424\n",
      "Iteration 890, the loss is 4.718647477189666, parameters k is 12.87220861889909 and b is -58.22354988289009\n",
      "Iteration 891, the loss is 4.718647308191515, parameters k is 12.872124014156007 and b is -58.22356964573594\n",
      "Iteration 892, the loss is 4.71864855373202, parameters k is 12.872039409412924 and b is -58.223589408581795\n",
      "Iteration 893, the loss is 4.718648934831046, parameters k is 12.872187018108576 and b is -58.22360917142765\n",
      "Iteration 894, the loss is 4.718648873563142, parameters k is 12.872102413365493 and b is -58.2236289342735\n",
      "Iteration 895, the loss is 4.718650561470587, parameters k is 12.872250022061145 and b is -58.22364869711935\n",
      "Iteration 896, the loss is 4.718650392472437, parameters k is 12.872165417318062 and b is -58.223668459965204\n",
      "Iteration 897, the loss is 4.718651067145956, parameters k is 12.872080812574978 and b is -58.223688222811056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 898, the loss is 4.718652019111973, parameters k is 12.87222842127063 and b is -58.22370798565691\n",
      "Iteration 899, the loss is 4.718651850113824, parameters k is 12.872143816527547 and b is -58.22372774850276\n",
      "Iteration 900, the loss is 4.718653260728772, parameters k is 12.872059211784464 and b is -58.22374751134861\n",
      "Iteration 901, the loss is 4.718653476753356, parameters k is 12.872206820480116 and b is -58.223767274194465\n",
      "Iteration 902, the loss is 4.718653580559891, parameters k is 12.872122215737033 and b is -58.22378703704032\n",
      "Iteration 903, the loss is 4.718655103392891, parameters k is 12.872269824432685 and b is -58.22380679988617\n",
      "Iteration 904, the loss is 4.718654934394742, parameters k is 12.872185219689602 and b is -58.22382656273202\n",
      "Iteration 905, the loss is 4.7186557741427055, parameters k is 12.872100614946518 and b is -58.223846325577874\n",
      "Iteration 906, the loss is 4.718656561034283, parameters k is 12.87224822364217 and b is -58.223866088423726\n",
      "Iteration 907, the loss is 4.718656392036128, parameters k is 12.872163618899087 and b is -58.22388585126958\n",
      "Iteration 908, the loss is 4.718657967725524, parameters k is 12.872079014156004 and b is -58.22390561411543\n",
      "Iteration 909, the loss is 4.718658018675662, parameters k is 12.872226622851656 and b is -58.22392537696128\n",
      "Iteration 910, the loss is 4.718658287556644, parameters k is 12.872142018108573 and b is -58.223945139807135\n",
      "Iteration 911, the loss is 4.718659645315203, parameters k is 12.872289626804225 and b is -58.22396490265299\n",
      "Iteration 912, the loss is 4.71865947631705, parameters k is 12.872205022061141 and b is -58.22398466549884\n",
      "Iteration 913, the loss is 4.71866048113946, parameters k is 12.872120417318058 and b is -58.22400442834469\n",
      "Iteration 914, the loss is 4.718661102956586, parameters k is 12.87226802601371 and b is -58.224024191190544\n",
      "Iteration 915, the loss is 4.718660933958438, parameters k is 12.872183421270627 and b is -58.2240439540364\n",
      "Iteration 916, the loss is 4.718662674722272, parameters k is 12.872098816527544 and b is -58.22406371688225\n",
      "Iteration 917, the loss is 4.718662560597972, parameters k is 12.872246425223196 and b is -58.2240834797281\n",
      "Iteration 918, the loss is 4.7186629945533936, parameters k is 12.872161820480112 and b is -58.22410324257395\n",
      "Iteration 919, the loss is 4.718664187237508, parameters k is 12.872309429175765 and b is -58.224123005419806\n",
      "Iteration 920, the loss is 4.718664018239358, parameters k is 12.872224824432681 and b is -58.22414276826566\n",
      "Iteration 921, the loss is 4.718665188136212, parameters k is 12.872140219689598 and b is -58.22416253111151\n",
      "Iteration 922, the loss is 4.718665644878893, parameters k is 12.87228782838525 and b is -58.22418229395736\n",
      "Iteration 923, the loss is 4.718665507967327, parameters k is 12.872203223642167 and b is -58.224202056803215\n",
      "Iteration 924, the loss is 4.718667271518425, parameters k is 12.872350832337819 and b is -58.22422181964907\n",
      "Iteration 925, the loss is 4.718667102520283, parameters k is 12.872266227594736 and b is -58.22424158249492\n",
      "Iteration 926, the loss is 4.718667701550142, parameters k is 12.872181622851652 and b is -58.22426134534077\n",
      "Iteration 927, the loss is 4.718668729159815, parameters k is 12.872329231547305 and b is -58.224281108186624\n",
      "Iteration 928, the loss is 4.718668560161665, parameters k is 12.872244626804221 and b is -58.224300871032476\n",
      "Iteration 929, the loss is 4.718669895132963, parameters k is 12.872160022061138 and b is -58.22432063387833\n",
      "Iteration 930, the loss is 4.718670186801203, parameters k is 12.87230763075679 and b is -58.22434039672418\n",
      "Iteration 931, the loss is 4.718670214964077, parameters k is 12.872223026013707 and b is -58.22436015957003\n",
      "Iteration 932, the loss is 4.718671813440733, parameters k is 12.872370634709359 and b is -58.224379922415885\n",
      "Iteration 933, the loss is 4.718671644442588, parameters k is 12.872286029966276 and b is -58.22439968526174\n",
      "Iteration 934, the loss is 4.718672408546896, parameters k is 12.872201425223192 and b is -58.22441944810759\n",
      "Iteration 935, the loss is 4.718673271082123, parameters k is 12.872349033918844 and b is -58.22443921095344\n",
      "Iteration 936, the loss is 4.718673102083975, parameters k is 12.872264429175761 and b is -58.224458973799294\n",
      "Iteration 937, the loss is 4.7186746021297115, parameters k is 12.872179824432678 and b is -58.224478736645146\n",
      "Iteration 938, the loss is 4.718674728723511, parameters k is 12.87232743312833 and b is -58.224498499491\n",
      "Iteration 939, the loss is 4.7186749219608295, parameters k is 12.872242828385247 and b is -58.22451826233685\n",
      "Iteration 940, the loss is 4.7186763553630495, parameters k is 12.872390437080899 and b is -58.2245380251827\n",
      "Iteration 941, the loss is 4.718676186364901, parameters k is 12.872305832337815 and b is -58.224557788028555\n",
      "Iteration 942, the loss is 4.718677115543645, parameters k is 12.872221227594732 and b is -58.22457755087441\n",
      "Iteration 943, the loss is 4.718677813004432, parameters k is 12.872368836290384 and b is -58.22459731372026\n",
      "Iteration 944, the loss is 4.718677644006292, parameters k is 12.872284231547301 and b is -58.22461707656611\n",
      "Iteration 945, the loss is 4.718679309126464, parameters k is 12.872199626804218 and b is -58.224636839411964\n",
      "Iteration 946, the loss is 4.718679270645818, parameters k is 12.87234723549987 and b is -58.22465660225782\n",
      "Iteration 947, the loss is 4.718679628957581, parameters k is 12.872262630756786 and b is -58.22467636510367\n",
      "Iteration 948, the loss is 4.718680897285356, parameters k is 12.872410239452439 and b is -58.22469612794952\n",
      "Iteration 949, the loss is 4.718680728287206, parameters k is 12.872325634709355 and b is -58.22471589079537\n",
      "Iteration 950, the loss is 4.7186818225404, parameters k is 12.872241029966272 and b is -58.224735653641225\n",
      "Iteration 951, the loss is 4.718682354926737, parameters k is 12.872388638661924 and b is -58.22475541648708\n",
      "Iteration 952, the loss is 4.7186821859285955, parameters k is 12.87230403391884 and b is -58.22477517933293\n",
      "Iteration 953, the loss is 4.718684016123213, parameters k is 12.872219429175757 and b is -58.22479494217878\n",
      "Iteration 954, the loss is 4.7186838125681225, parameters k is 12.87236703787141 and b is -58.224814705024635\n",
      "Iteration 955, the loss is 4.718684335954332, parameters k is 12.872282433128326 and b is -58.22483446787049\n",
      "Iteration 956, the loss is 4.718685439207659, parameters k is 12.872430041823979 and b is -58.22485423071634\n",
      "Iteration 957, the loss is 4.7186852702095115, parameters k is 12.872345437080895 and b is -58.22487399356219\n",
      "Iteration 958, the loss is 4.718686529537147, parameters k is 12.872260832337812 and b is -58.22489375640804\n",
      "Iteration 959, the loss is 4.71868689684905, parameters k is 12.872408441033464 and b is -58.224913519253896\n",
      "Iteration 960, the loss is 4.718686849368264, parameters k is 12.87232383629038 and b is -58.22493328209975\n",
      "Iteration 961, the loss is 4.718688523488584, parameters k is 12.872471444986033 and b is -58.2249530449456\n",
      "Iteration 962, the loss is 4.71868835449043, parameters k is 12.87238684024295 and b is -58.22497280779145\n",
      "Iteration 963, the loss is 4.718689042951085, parameters k is 12.872302235499866 and b is -58.224992570637305\n",
      "Iteration 964, the loss is 4.7186899811299705, parameters k is 12.872449844195518 and b is -58.22501233348316\n",
      "Iteration 965, the loss is 4.718689812131823, parameters k is 12.872365239452435 and b is -58.22503209632901\n",
      "Iteration 966, the loss is 4.718691236533904, parameters k is 12.872280634709352 and b is -58.22505185917486\n",
      "Iteration 967, the loss is 4.718691438771356, parameters k is 12.872428243405004 and b is -58.225071622020714\n",
      "Iteration 968, the loss is 4.718691556365016, parameters k is 12.87234363866192 and b is -58.225091384866566\n",
      "Iteration 969, the loss is 4.71869306541089, parameters k is 12.872491247357573 and b is -58.22511114771242\n",
      "Iteration 970, the loss is 4.718692896412746, parameters k is 12.87240664261449 and b is -58.22513091055827\n",
      "Iteration 971, the loss is 4.718693749947837, parameters k is 12.872322037871406 and b is -58.22515067340412\n",
      "Iteration 972, the loss is 4.718694523052279, parameters k is 12.872469646567058 and b is -58.225170436249975\n",
      "Iteration 973, the loss is 4.718694354054129, parameters k is 12.872385041823975 and b is -58.22519019909583\n",
      "Iteration 974, the loss is 4.718695943530656, parameters k is 12.872300437080892 and b is -58.22520996194168\n",
      "Iteration 975, the loss is 4.718695980693664, parameters k is 12.872448045776544 and b is -58.22522972478753\n",
      "Iteration 976, the loss is 4.718696263361772, parameters k is 12.87236344103346 and b is -58.225249487633384\n",
      "Iteration 977, the loss is 4.718697607333198, parameters k is 12.872511049729113 and b is -58.225269250479236\n",
      "Iteration 978, the loss is 4.718697438335056, parameters k is 12.87242644498603 and b is -58.22528901332509\n",
      "Iteration 979, the loss is 4.718698456944588, parameters k is 12.872341840242946 and b is -58.22530877617094\n",
      "Iteration 980, the loss is 4.718699064974585, parameters k is 12.872489448938598 and b is -58.22532853901679\n",
      "Iteration 981, the loss is 4.718698895976441, parameters k is 12.872404844195515 and b is -58.225348301862645\n",
      "Iteration 982, the loss is 4.718700650527399, parameters k is 12.872320239452431 and b is -58.2253680647085\n",
      "Iteration 983, the loss is 4.718700522615976, parameters k is 12.872467848148084 and b is -58.22538782755435\n",
      "Iteration 984, the loss is 4.718700970358523, parameters k is 12.872383243405 and b is -58.2254075904002\n",
      "Iteration 985, the loss is 4.718702149255512, parameters k is 12.872530852100653 and b is -58.225427353246054\n",
      "Iteration 986, the loss is 4.7187019802573635, parameters k is 12.87244624735757 and b is -58.22544711609191\n",
      "Iteration 987, the loss is 4.7187031639413375, parameters k is 12.872361642614486 and b is -58.22546687893776\n",
      "Iteration 988, the loss is 4.718703606896899, parameters k is 12.872509251310138 and b is -58.22548664178361\n",
      "Iteration 989, the loss is 4.7187034837724555, parameters k is 12.872424646567055 and b is -58.22550640462946\n",
      "Iteration 990, the loss is 4.718705233536428, parameters k is 12.872572255262707 and b is -58.225526167475316\n",
      "Iteration 991, the loss is 4.718705064538286, parameters k is 12.872487650519624 and b is -58.22554593032117\n",
      "Iteration 992, the loss is 4.718705677355274, parameters k is 12.87240304577654 and b is -58.22556569316702\n",
      "Iteration 993, the loss is 4.718706691177813, parameters k is 12.872550654472192 and b is -58.22558545601287\n",
      "Iteration 994, the loss is 4.718706522179665, parameters k is 12.872466049729109 and b is -58.225605218858725\n",
      "Iteration 995, the loss is 4.71870787093809, parameters k is 12.872381444986026 and b is -58.22562498170458\n",
      "Iteration 996, the loss is 4.718708148819206, parameters k is 12.872529053681678 and b is -58.22564474455043\n",
      "Iteration 997, the loss is 4.718708190769207, parameters k is 12.872444448938595 and b is -58.22566450739628\n",
      "Iteration 998, the loss is 4.71870977545874, parameters k is 12.872592057634247 and b is -58.225684270242134\n",
      "Iteration 999, the loss is 4.718709606460588, parameters k is 12.872507452891163 and b is -58.225704033087986\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100   # random.random()产生 0 到 1 之间的随机浮点数\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "iteration_num = 1000\n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16edd940>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x16f0cd68>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
