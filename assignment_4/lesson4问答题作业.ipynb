{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does a neuron compute?\n",
    "\n",
    "   A : 神经元有一些输入信号，在神经元里对输入信号做一些简单的处理，然后输出。\n",
    "     神经网络接受一些输入，做一些线性的加和计算，然后通过某一个函数计算，最后得到一个输出。\n",
    "   \n",
    "   \n",
    "   \n",
    "2. Why we use non-linear activation funcitons in neural networks?\n",
    "\n",
    "   A:使用非线性激活函数可以保证神经网络在深度上是有意义的，线性激活函数会导致一个一层的神经网络。\n",
    "   \n",
    "   +  不同角度回答：\n",
    "\n",
    "   > 1.如果隐藏层使用线性函数，那么多个线性函数的叠加一定可以由一个线性函数来表示，这样的话隐藏层的深度就没有意义了。而且，我们希望描述输入X与输出y的复杂关系，但是用线性函数作为激活函数无法让神经网络去逼近复杂函数\n",
    "   \n",
    "   > 2.很多时候神经元的输出会很大，然后再往下传递会对系统的算力有比较大的需求；另外我们在处理特定问题的时候希望把输出控制在某一范围内，比如说概率问题，我们就可以用sigmoid可以把输出映射在(0,1)。简单的说就是激活函数有时需要将神经元输出压缩到某一有界区间内，这时用线性函数是无法做到的。\n",
    "   \n",
    "   \n",
    "3. What is the 'Logistic Loss' ?\n",
    "\n",
    "   A:$$L = -ylog(\\hat{y})-(1-y)log(1-\\hat{y})$$\n",
    "   \n",
    "   \n",
    "4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?      \n",
    "答：选C。sigmoid函数把概率变成0到1之间\n",
    "\n",
    "\n",
    "A. ReLU\n",
    "\n",
    "B. Leaky ReLU\n",
    "\n",
    "C. sigmoid\n",
    "\n",
    "D. tanh\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "5. Why we don't use zero initialization for all parameters ?\n",
    "\n",
    "   A：w初始化为0会造成神经元对称性问题，使得每个节点计算的东西一样。所以一般不把w初始化为0。\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Can you implement the softmax function using python ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x -= np.max(x, axis = 1, keepdims = True)  #为了稳定地计算softmax概率， 一般会减掉最大的那个元素\n",
    "    print(\"减去行最大值 ：\\n\", x)\n",
    "    \n",
    "    x = np.exp(x) / np.sum(np.exp(x), axis = 1, keepdims = True)  # axis = 1, 行\n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 ：\n",
      " [[3 1 1]\n",
      " [3 4 2]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(low = 1, high = 5, size = (2, 3))  #生成一个2x3的矩阵，取值范围在1-5之间\n",
    "print(\"原始 ：\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "减去行最大值 ：\n",
      " [[ 0 -2 -2]\n",
      " [-1  0 -2]]\n",
      "变换后 ：\n",
      " [[0.78698604 0.10650698 0.10650698]\n",
      " [0.24472847 0.66524096 0.09003057]]\n"
     ]
    }
   ],
   "source": [
    "x_ = softmax(x)\n",
    "print(\"变换后 ：\\n\", x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
